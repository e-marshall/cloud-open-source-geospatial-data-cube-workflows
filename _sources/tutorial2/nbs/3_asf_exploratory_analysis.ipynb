{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1cbba21-1acd-42df-8642-d03abe72c7b2",
   "metadata": {},
   "source": [
    "{{title_s1_3}}\n",
    "\n",
    "Now that we have read in and organized the stack of Sentinel-1 RTC images, let's take a look at the data.\n",
    "\n",
    ":::{admonition} ASF data access options\n",
    "The steps shown in this notebook involve downloading and extracting large volumes of data. **It is not necessary to do this to follow the rest of the content in the tutorial**. We include the demonstration for the purposes of completeness and to help users who may be in this situation.\n",
    "\n",
    "For more information on different options for downloading data locally, see the [Introduction](../s1_intro.md#different-ways-to-use-this-tutorial).\n",
    ":::\n",
    "\n",
    "\n",
    "::::{tab-set}\n",
    ":::{tab-item} Outline\n",
    "\n",
    "(content.section_A)=\n",
    "**[A. Read and prepare data](#a-read-and-prepare-data)**  \n",
    "- {{a1_s1_nb3}}  \n",
    "\n",
    "(content.section_B)=\n",
    "**[B. Layover-shadow mask](#b-layover-shadow-mask)**  \n",
    "\n",
    "(content.section_C)=\n",
    "**[C. Orbital direction](#c-orbital-direction)**  \n",
    "- {{c1_s1_nb3}}\n",
    "- {{c2_s1_nb3}}\n",
    "\n",
    "(content.section_D)=\n",
    "**[D. Duplicate time steps](#d-handling-duplicate-time-steps)**  \n",
    "- {{d1_s1_nb3}}\n",
    "- {{d2_s1_nb3}}\n",
    "- {{d3_s1_nb3}}\n",
    "\n",
    "(content.section_E)=\n",
    "**[E. Data Visualization](#e-data-visualization)**\n",
    "- {{e1_s1_nb3}}\n",
    "- {{e2_s1_nb3}}\n",
    "- {{e3_s1_nb3}}\n",
    "\n",
    ":::\n",
    ":::{tab-item} Learning goals  \n",
    "{{concepts}}\n",
    "- Spatial joins of raster and vector data.  \n",
    "- Visualize raster data.  \n",
    "- Use raster metadata to aid interpretation of backscatter imagery.  \n",
    "- Examine data quality using provided layover-shadow maps.  \n",
    "- Identify and remove duplicate time step observations.  \n",
    "\n",
    "{{techniques}}\n",
    "- Clip raster data cube using vector data with [`rioxarray.clip()`](https://corteva.github.io/rioxarray/html/examples/clip_geom.html).  \n",
    "- Using `xr.groupby()` for [grouped statistics](https://docs.xarray.dev/en/stable/user-guide/groupby.html).  \n",
    "- Reorganizing data with `xr.Dataset.reindex()`.  \n",
    "- Visualizing multiple facets of the data using `FacetGrid`\n",
    "\n",
    "\n",
    ":::\n",
    "::::\n",
    "\n",
    ":::{admonition} ASF Data Access\n",
    "You can download the RTC-processed backscatter time series [here](https://zenodo.org/record/7236413#.Y1rNi37MJ-0). For more detail, see [tutorial data](../background/tutorial_data.md#sentinel-1-rtc-datasets) and the [notebook](1_read_asf_data.ipynb) on reading ASF Sentinel-1 RTC data into memory.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f4a3f-1adc-4f61-979a-94ec5de93f38",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "#%xmode minimal\n",
    "import geopandas as gpd\n",
    "import hvplot.xarray\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import rioxarray as rio\n",
    "import warnings\n",
    "import xarray as xr\n",
    "\n",
    "import s1_tools\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99653d20",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "tutorial2_dir = pathlib.Path(cwd).parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ae6a2",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3840e-f4a6-4bb3-ad23-1e83a0e5aa0b",
   "metadata": {},
   "source": [
    "## A. Read and prepare data\n",
    "\n",
    "We'll go through the steps shown in [metadata wrangling](2_wrangle_metadata.ipynb), but this time,  combined into one function from `s1_tools`. \n",
    "\n",
    ":::{attention} \n",
    "If you are following along on your own computer, you **must** specify `'timeseries_type'` below: \n",
    "1. Set `timeseries_type` to `'full'` or `'subset'` depending on if you are using the full time series (103 files) or the subset time series (5 files).\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ddcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_type = 'full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac699f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_vrt_path = f\"../data/{timeseries_type}_timeseries/vrt_files/s1_stack_vv.vrt\"\n",
    "vh_vrt_path = f\"../data/{timeseries_type}_timeseries/vrt_files/s1_stack_vh.vrt\"\n",
    "ls_vrt_path = f\"../data/{timeseries_type}_timeseries/vrt_files/s1_stack_ls_map.vrt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43986980",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_data_cube = s1_tools.metadata_processor(\n",
    "    vv_path=vv_vrt_path, \n",
    "    vh_path=vh_vrt_path, \n",
    "    ls_path=ls_vrt_path,\n",
    "    timeseries_type = timeseries_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d54aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_data_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c975234",
   "metadata": {},
   "source": [
    "### {{a1_s1_nb3}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9c132",
   "metadata": {},
   "source": [
    "Until now, we've kept the full spatial extent of the dataset. This hasn't been a problem because all of our operations have been lazy. Now, we'd like to visualize the dataset in ways that require eager instead of lazy computation. We subset the data cube to a smaller area to focus on a location interest to make computation more less computationally-intensive.\n",
    "\n",
    "Later notebooks use a different Sentinel-1 RTC dataset that is accessed for a smaller area of interest. Clip the current data cube to that spatial footprint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "487ae5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read vector data\n",
    "pc_aoi = gpd.read_file(\n",
    "    \"https://github.com/e-marshall/sentinel1_rtc/raw/main/hma_rtc_aoi.geojson\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df69a0a",
   "metadata": {},
   "source": [
    "Visualize location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b48c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_aoi.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ad582",
   "metadata": {},
   "source": [
    "Check the CRS and ensure it matches that of the raster data cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4e708f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert asf_data_cube.rio.crs == pc_aoi.crs, (\n",
    "    f\"Expected: {asf_data_cube.rio.crs}, received: {pc_aoi.crs}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883fddf",
   "metadata": {},
   "source": [
    "Clip the raster data cube by the extent of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a85c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube = asf_data_cube.rio.clip(pc_aoi.geometry, pc_aoi.crs)\n",
    "clipped_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0765626a",
   "metadata": {},
   "source": [
    "Persist the dataset in memory so that later operations will be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0266a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube = clipped_cube.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b28ee7",
   "metadata": {},
   "source": [
    "Great, we've gone from an object where each 3-d variable is ~ 90 GB to one where each 3-d variable is ~45 MB, this will be much easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e8cb7",
   "metadata": {},
   "source": [
    "## B. Layover-shadow mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f06d845",
   "metadata": {},
   "source": [
    "As discussed in previous notebooks, every Sentinel-1 scene comes with an associated layover shadow mask GeoTIFF file. This map describes the presence of layover, shadow and slope angle conditions that can impact backscatter values in a scene, which is especially important to consider in high-relief settings with more potential for geometric distortions. \n",
    "\n",
    "The following information is copied from the README file that accompanies each scene: \n",
    "\n",
    "```{**Layover-shadow mask**}\n",
    "\n",
    "The layover/shadow mask indicates which pixels in the RTC image have been affected by layover and shadow. This layer is tagged with _ls_map.tif\n",
    "\n",
    "The pixel values are generated by adding the following values together to indicate which layover and shadow effects are impacting each pixel:\n",
    "0.  Pixel not tested for layover or shadow\n",
    "1.  Pixel tested for layover or shadow\n",
    "2.  Pixel has a look angle less than the slope angle\n",
    "4.  Pixel is in an area affected by layover\n",
    "8.  Pixel has a look angle less than the opposite of the slope angle\n",
    "16. Pixel is in an area affected by shadow\n",
    "\n",
    "There are 17 possible different pixel values, indicating the layover, shadow, and slope conditions present added together for any given pixel._\n",
    "\n",
    "The values in each cell can range from 0 to 31:\n",
    "0.  Not tested for layover or shadow\n",
    "1.  Not affected by either layover or shadow\n",
    "3.  Look angle < slope angle\n",
    "5.  Affected by layover\n",
    "7.  Affected by layover; look angle < slope angle\n",
    "9.  Look angle < opposite slope angle\n",
    "11. Look angle < slope and opposite slope angle\n",
    "13. Affected by layover; look angle < opposite slope angle\n",
    "15. Affected by layover; look angle < slope and opposite slope angle\n",
    "17. Affected by shadow\n",
    "19. Affected by shadow; look angle < slope angle\n",
    "21. Affected by layover and shadow\n",
    "23. Affected by layover and shadow; look angle < slope angle\n",
    "25. Affected by shadow; look angle < opposite slope angle\n",
    "27. Affected by shadow; look angle < slope and opposite slope angle\n",
    "29. Affected by shadow and layover; look angle < opposite slope angle\n",
    "31. Affected by shadow and layover; look angle < slope and opposite slope angle\n",
    "\n",
    "```\n",
    "\n",
    "The ASF RTC image [product guide](https://hyp3-docs.asf.alaska.edu/guides/rtc_product_guide/) has detailed descriptions of how the data is processed and what is included in the processed dataset.\n",
    "\n",
    "The `layover-shadow` variable provides categorical information so we'll use a qualitative colormap to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84010653",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cmap = plt.get_cmap('tab20b', lut=32)\n",
    "time_step1 = 10\n",
    "time_step2 = 11\n",
    "\n",
    "if timeseries_type == 'subset':\n",
    "    time_step1 = 1\n",
    "    time_step2 = 3\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12,7), layout='constrained')\n",
    "\n",
    "clipped_cube.isel(acq_date=time_step1).ls.plot(ax=axs[0], \n",
    "                                       cmap=cat_cmap,\n",
    "                                       cbar_kwargs=({'label':None}),\n",
    "                                       vmin=0, vmax=31)\n",
    "\n",
    "clipped_cube.isel(acq_date=time_step2).ls.plot(ax=axs[1], \n",
    "                                       cmap=cat_cmap,\n",
    "                                       cbar_kwargs=({'label':None}),\n",
    "                                       vmin=0, vmax=31)\n",
    "\n",
    "date1 = str(clipped_cube.isel(acq_date=time_step1).acq_date.data)[:-19]\n",
    "date2 = str(clipped_cube.isel(acq_date=time_step2).acq_date.data)[:-19]\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Layover shadow map: {date1}, and {date2}\",\n",
    "    y=1.05\n",
    ")\n",
    "for i in range(len(axs)):\n",
    "    axs[i].set_xlabel(None)\n",
    "    axs[i].set_ylabel(None)\n",
    "    axs[i].tick_params(axis='x', labelrotation=45);\n",
    "\n",
    "axs[0].set_title(date1)\n",
    "axs[1].set_title(date2)\n",
    "fig.supylabel('y coordinate of projection (m)')\n",
    "fig.supxlabel('x coordinate of projection (m)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b3cc4",
   "metadata": {},
   "source": [
    "It looks like there are areas affected by different types of distortion on different dates. For example, in the lower left quadrant, there is a region that is blue (5 - affected by layover) on 6/7/2021 but much of that area appears to be in radar shadow on 6/10/2021. This pattern is present throughout much of the scene with portions of area that are affected by layover in one acquisition in shadow in the next acquisition. This is due to the viewing geometries of different orbital passes: one of the above scenes  was collected during an ascending pass of the satellite and one during a descending pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de6f05",
   "metadata": {},
   "source": [
    "(content:orbital_dir_section)=\n",
    "## C. Orbital direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e8bcc",
   "metadata": {},
   "source": [
    "Sentinel-1 is a right-looking sensor and it images areas on Earth's surface in orbits when it is moving N-S (a descending orbit) and S-N (an ascending orbit). It images the same footprint on both passes but from different directions. The data coverage map below illustrates these directional passes, it can be found online [here](https://asf.alaska.edu/daac/sentinel-1-acquisition-maps/)\n",
    "\n",
    "```{image} ../imgs/slc_coverage_asf.png\n",
    ":align center\n",
    "```\n",
    "ASF Sentinel-1 Cumulative coverage map.\n",
    "\n",
    "In areas of high-relief topography such as the area we're observing, there can be strong terrain distortion effects such as layover and shadow. These are some of the distortions that RTC processing corrects, but sometimes it is not possible to reliably extract backscatter in the presence of strong distortions. The above image shows the layover-shadow map for an ascending and a descending image side-by-side, which is why different areas are affected by layover (5) and shadow (17) in each. \n",
    "\n",
    "Thanks to all the setup work we did in the previous notebook, we can quickly confirm that all of the observations were taken at two times of day, corresponding to ascending and descending passes of the satellite, and that the time steps shown above were taken at different times of day.\n",
    "\n",
    ":::{note}\n",
    "The acquisition time of Sentinel-1 images is not in local time.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Hour of day of acquisition {time_step1}: \",\n",
    "    clipped_cube.isel(acq_date=time_step1).acq_date.dt.hour.data,\n",
    ")\n",
    "print(\n",
    "    f\"Hour of day of acquisition {time_step2}: \",\n",
    "    clipped_cube.isel(acq_date=time_step2).acq_date.dt.hour.data,\n",
    ")\n",
    "clipped_cube.acq_date.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b6dc3",
   "metadata": {},
   "source": [
    "### {{c1_s1_nb3}}\n",
    "\n",
    "In this example, it was relatively simple to determine one pass from another, but it is less straightforward to know if a pass is ascending or descending. The timing of these passes depends on the location on earth of the image. \n",
    "\n",
    "In the location covered by this dataset, ascending passes correspond to an acquisition time roughly 0:00 UTC and descending passes correspond to approximately 12:00 UTC. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409f2f2",
   "metadata": {},
   "source": [
    "### {{c2_s1_nb3}}\n",
    "This is another example of time-varying metadata, so it should be stored as a coordinate variable. Use [`xr.where()`](https://docs.xarray.dev/en/stable/generated/xarray.where.html) to assign the correct orbital direction value depending on an observation's acquisition time and then assign it as a coordinate variable to the clipped raster data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7607b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube.coords[\"orbital_dir\"] = (\n",
    "    \"acq_date\",\n",
    "    xr.where(clipped_cube.acq_date.dt.hour.data == 0, \"asc\", \"desc\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba95d2",
   "metadata": {},
   "source": [
    "## D. Duplicate time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7407bada",
   "metadata": {},
   "source": [
    "If we take a closer look at the ASF dataset, we can see that there are a few scenes from identical acquisitions (this is apparent in `acq_date` and more specifically in `product_id`). Let's examine these and see what's going on. \n",
    "\n",
    ":::{note}\n",
    "This section **will not work** if you're using the subset timeseries.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2580b08",
   "metadata": {},
   "source": [
    "### {{d1_s1_nb3}}\n",
    "\n",
    "First we'll extract the `data_take_ID` from the Sentinel-1 granule ID: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cfec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube.data_take_ID.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f895feee",
   "metadata": {},
   "source": [
    "Let's look at the number of unique elements using [`np.unique()`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_take_ids_ls = clipped_cube.data_take_ID.data.tolist()\n",
    "data_take_id_set = np.unique(clipped_cube.data_take_ID)\n",
    "len(data_take_id_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181b6456",
   "metadata": {},
   "source": [
    "Interesting - it looks likethere are only 96 unique elements. Let's figure out which are duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1babb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate(input_ls):\n",
    "    return list(set([x for x in input_ls if input_ls.count(x) > 1]))\n",
    "\n",
    "duplicate_ls = duplicate(data_take_ids_ls)\n",
    "duplicate_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a310429",
   "metadata": {},
   "source": [
    "These are the data take IDs that are duplicated in the dataset. We now want to subset the xarray object to only include these data take IDs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b56ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_duplicate_cond = clipped_cube.data_take_ID.isin(duplicate_ls)\n",
    "asf_duplicate_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0841d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_cube = clipped_cube.where(asf_duplicate_cond == True, drop=True)\n",
    "duplicates_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ffdd8",
   "metadata": {},
   "source": [
    "### {{d2_s1_nb3}}\n",
    "\n",
    "Great, now we have a 12-time step Xarray object that contains only the duplicate data takes. Let's see what it looks like. We can use `xr.FacetGrid` objects to plot all of the arrays at once.\n",
    "\n",
    "Before we make a FacetGrid plot, we need to make a change to the dataset. FacetGrid takes a column and expands the levels of the provided dimension into individual sub-plots (a small multiples plot). We're looking at the duplicate time steps, meaning the elements of the `acq_date` dimension are non-unique. FacetGrid expects unique values along the specified coordinate array. If we were to directly call: \n",
    "```python\n",
    "fg = duplicates_cube.vv.plot(col=\"acq_date\", col_wrap=4)\n",
    "``` \n",
    "We would receive the following error: \n",
    "```\n",
    "ValueError: Coordinates used for faceting cannot contain repeated (nonunique) values.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d4ff9",
   "metadata": {},
   "source": [
    "Renaming the dimensions of `duplicates_cube` with [`xr.rename_dims()`](https://docs.xarray.dev/en/latest/generated/xarray.Dataset.rename_dims.html) demotes the `acq_date` coordinate array to non-dimensional coordinate and replaces it with `step` an array of integers. Because these are unique, we can make a FaceGrid plot with the `step` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c06d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_cube.rename_dims({'acq_date':'step'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63393b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = duplicates_cube.rename_dims({\"acq_date\": \"step\"}).vv.plot(col=\"step\", col_wrap=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228f8518",
   "metadata": {},
   "source": [
    "Interesting, it looks like there's only really data for the 0, 2, 4, 7 and 9 elements of the list of duplicates. It could be that the processing of these files was interrupted and then restarted, producing extra empty arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886eb37f",
   "metadata": {},
   "source": [
    "### {{d3_s1_nb3}}\n",
    "\n",
    "To drop these arrays, extract the product ID (the only variable that is unique among the duplicates) of each array we'd like to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58678828",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ls = [1, 3, 5, 6, 8, 10, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f5e524",
   "metadata": {},
   "source": [
    "We can use xarray's `.isel()` method, `.xr.DataArray.isin()`, `xr.Dataset.where()`, and list comprehension to efficiently subset the time steps we want to keep: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fccbab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_product_id_ls = duplicates_cube.isel(acq_date=drop_ls).product_id.data\n",
    "drop_product_id_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9bab7",
   "metadata": {},
   "source": [
    "Using this list, we want to drop all of the elements of `clipped_cube` where product Id is one of the values in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_cond = ~clipped_cube.product_id.isin(drop_product_id_ls)\n",
    "clipped_cube = clipped_cube.where(duplicate_cond == True, drop=True)\n",
    "clipped_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14382448",
   "metadata": {},
   "source": [
    "## E. Data visualization\n",
    "\n",
    "Now that we've visualized and taken a closer look at metadata such as the layover shadow map and orbital direction, let's focus on backscatter variability. In the plotting calls in this notebook, we'll use a function in `s1_tools.py` that applies a logarithmic transformation to the data. This makes it easier to visualize variability, however it is important to apply it as a last step before visualizing and to not perform statistical calculations on the log-transformed data.\n",
    "\n",
    ":::{admonition} A note on visualizing SAR data\n",
    "The measurements that we're provided in the RTC dataset are in intensity, or power, scale. Often, to visualize SAR backscatter, the data is converted from power to normalized radar cross section (the backscatter coefficient). This is in decibel (dB) units, meaning a log transform has been applied. This transformation makes it easier to visualize variability but it is important not to calculate summary statistics on log-transformed data as it will be distorted. You can read more about these concepts [here](https://hyp3-docs.asf.alaska.edu/guides/introduction_to_sar/#sar-scale).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c9fdd6",
   "metadata": {},
   "source": [
    "### {{e1_s1_nb3}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c66fe",
   "metadata": {},
   "source": [
    "This section examines two approaches of plotting VV and VH backscatter side by side that can have important consequences when visualizing and interpreting data. \n",
    "\n",
    "Currently, VV and VH are two data variables of the data cube that both exist along x,y and time dimensions. If we plot them both as individual subplots, we'll see two subplots, each with their own colormap. If you look closely, you can see that the colormaps are not on the same scale. We need to be careful when interpreting these images and comparing backscatter in the VV and VH images in this situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(12,7),layout='constrained')\n",
    "\n",
    "s1_tools.power_to_db(clipped_cube['vv'].mean(dim='acq_date')).plot(ax=ax[0],\n",
    "                                                                   cmap=plt.cm.Greys_r, \n",
    "                                                                   cbar_kwargs=({'label':'dB'}))\n",
    "s1_tools.power_to_db(clipped_cube['vh'].mean(dim='acq_date')).plot(ax=ax[1],\n",
    "                                                                   cmap=plt.cm.Greys_r,\n",
    "                                                                   cbar_kwargs=({'label':'dB'}))\n",
    "\n",
    "for i in range(len(ax)):\n",
    "    ax[i].set_ylabel(None)\n",
    "    ax[i].set_xlabel(None)\n",
    "    ax[i].tick_params(axis=\"x\", labelrotation=45)\n",
    "fig.suptitle('Mean backscatter over time')\n",
    "ax[0].set_title('VV')\n",
    "ax[1].set_title('VH')\n",
    "fig.supylabel('y coordinate of projection (m)')\n",
    "fig.supxlabel('x coordinate of projection (m)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d4dd1",
   "metadata": {},
   "source": [
    "There are two ways to fix this: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ebcda0",
   "metadata": {},
   "source": [
    "#### Specify min and max values in plotting call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0138e9",
   "metadata": {},
   "source": [
    "We could normalize the backscatter ranges for both variables by manually specifying a minimum and a maximum across both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58294317",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_min = np.array([\n",
    "    s1_tools.power_to_db(clipped_cube['vv'].mean(dim='acq_date')).min(),\n",
    "    s1_tools.power_to_db(clipped_cube['vh'].mean(dim='acq_date')).min(),\n",
    "]).min()\n",
    "\n",
    "global_max = np.array([\n",
    "    s1_tools.power_to_db(clipped_cube['vv'].mean(dim='acq_date')).max(),\n",
    "    s1_tools.power_to_db(clipped_cube['vh'].mean(dim='acq_date')).max()\n",
    "]).max()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12,7),layout='constrained')\n",
    "\n",
    "s1_tools.power_to_db(clipped_cube['vv'].mean(dim='acq_date')).plot(ax=ax[0],\n",
    "                                                                   cmap=plt.cm.Greys_r, \n",
    "                                                                   vmin=global_min,\n",
    "                                                                   vmax=global_max,\n",
    "                                                                   cbar_kwargs=({'label':'dB'}))\n",
    "s1_tools.power_to_db(clipped_cube['vh'].mean(dim='acq_date')).plot(ax=ax[1],\n",
    "                                                                   cmap=plt.cm.Greys_r,\n",
    "                                                                   vmin=global_min,\n",
    "                                                                   vmax=global_max,\n",
    "                                                                   cbar_kwargs=({'label':'dB'}))\n",
    "\n",
    "for i in range(len(ax)):\n",
    "    ax[i].set_ylabel(None)\n",
    "    ax[i].set_xlabel(None)\n",
    "    ax[i].tick_params(axis=\"x\", labelrotation=45)\n",
    "fig.suptitle('Mean backscatter over time')\n",
    "ax[0].set_title('VV')\n",
    "ax[1].set_title('VH')\n",
    "fig.supylabel('y coordinate of projection (m)')\n",
    "fig.supxlabel('x coordinate of projection (m)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ab4dc5",
   "metadata": {},
   "source": [
    "#### Expand dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63436945",
   "metadata": {},
   "source": [
    "We could convert the VV and VH data from being represented as data variables to as elements of a band dimension. This would let us use Xarray's FacetGrid plotting that creates small multiples along a given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "160a09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube_da = clipped_cube.to_array(dim='band')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f43b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = s1_tools.power_to_db(clipped_cube_da.mean(dim='acq_date')).plot(col='band',\n",
    "                                                                cmap=plt.cm.Greys_r, \n",
    "                                                                cbar_kwargs=({'label':'dB'}))\n",
    "\n",
    "f.fig.suptitle('Mean backscatter over time for VV and VH polarizations with FacetGrid')\n",
    "f.fig.set_figheight(7)\n",
    "f.fig.set_figwidth(12);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1353e2",
   "metadata": {},
   "source": [
    "The area that we're looking at is in the mountainous region on the border between the Sikkim region of India and China. There are four north-facing glacier visible in the image, each with a lake at the toe. Bodies of water like lakes tend to appear dark in C-band SAR images because water is smooth with respect to the wavelength of the signal, meaning that most of the emitted signal is scattered away from the sensor. where surfaces that are rough at the scale of C-band wavelength, more signal is returned to the sensor and the backscatter image is brighter. For much more detail on interpreting SAR imagery, see the resources linked in the Sentinel-1 section of the [tutorial data](../../background/tutorial_data.md) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d41abc9-9d67-4ef7-8a23-ea2699cb8920",
   "metadata": {},
   "source": [
    "### {{e2_s1_nb3}}\n",
    "\n",
    "Now let's look at how backscatter may vary seasonally for a single polarization (for more on time-related GroupBy operations see the [Xarray User Guide](https://docs.xarray.dev/en/stable/user-guide/time-series.html#resampling-and-grouped-operations)). This is an example of a 'split-apply-combine' operation, where a dataset is split into groups (in this case, time steps are split into seasons), an operation is applied (in this case, the mean is calculated) and then the groups are combined into a new object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d932cee5-a9d4-49da-95bc-386dffcad6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube_gb = clipped_cube.groupby(\"acq_date.season\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02768bb",
   "metadata": {},
   "source": [
    "The temporal dimension of the new object has an element for each season rather than an element for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee55f9-130b-46bd-a4ba-6d5a24669885",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b05f5e-aca3-4ec0-938a-3c775724a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order seasons correctly\n",
    "clipped_cube_gb = clipped_cube_gb.reindex({\"season\": [\"DJF\", \"MAM\", \"JJA\", \"SON\"]})\n",
    "clipped_cube_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f020fd",
   "metadata": {},
   "source": [
    "Visualize mean backscatter in each season:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a662aa5-eb13-44fe-8c61-c9b38a96338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_vv = s1_tools.power_to_db(clipped_cube_gb.vv).plot(col=\"season\", cmap=plt.cm.Greys_r,\n",
    "                                                       cbar_kwargs=({'label':'dB'}));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd42b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_vh = s1_tools.power_to_db(clipped_cube_gb.vh).plot(col='season',\n",
    "                                                      cmap=plt.cm.Greys_r,\n",
    "                                                      cbar_kwargs=({'label':'dB'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb2508",
   "metadata": {},
   "source": [
    "The glacier surfaces appear much darker during the summer months than other seasons, especially in the lower reaches of the glaciers. Like the lake surfaces above, this suggests largely specular reflection where no signal returns in the incident direction. It could be that during the summer months, enough liquid water is present at the glacier surface to produce this scattering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fd823",
   "metadata": {},
   "source": [
    "### {{e3_s1_nb3}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "\n",
    "s1_tools.power_to_db(clipped_cube['vv'].median(dim=['x','y'])).plot.scatter(ax=ax, label='VV', c='b')\n",
    "s1_tools.power_to_db(clipped_cube['vh'].median(dim=['x','y'])).plot.scatter(ax=ax, label='VH', c='r')\n",
    "fig.legend(loc='center right')\n",
    "\n",
    "fig.suptitle('Backscatter variability over time, VV and VH polarizations')\n",
    "ax.set_title(None)\n",
    "ax.set_ylabel('dB')\n",
    "ax.set_xlabel('Time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12f2d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881ad14",
   "metadata": {},
   "source": [
    "To take a look at backscatter variability in more detail, you could use [`hvplot`](https://tutorial.xarray.dev/intermediate/hvplot.html) to make an interactive plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a69f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_plot = s1_tools.power_to_db(clipped_cube['vv'].mean(dim=['x','y'])).hvplot.scatter(x='acq_date')\n",
    "vh_plot = s1_tools.power_to_db(clipped_cube['vh'].mean(dim=['x','y'])).hvplot.scatter(x='acq_date')\n",
    "\n",
    "vv_plot * vh_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130494d",
   "metadata": {},
   "source": [
    "{{conclusion}}\n",
    "\n",
    "In this notebook, we demonstrated how to use the data cube that we assembled in the previous notebooks. We saw various ways that having metadata accessible and attached to the correct dimensions of the data cube made learning about teh dataset much smoother and more efficient than it would otherwise be. \n",
    "\n",
    "In the next notebook, we'll work with a different Sentinel-1 RTC dataset. We'll write this dataset to disk in order to use it in the final notebook of the tutorial, a comparison of two datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22671cc4-5e5e-40c1-90d2-2a74290b2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_cube.to_zarr(f\"../data/{timeseries_type}_timeseries/intermediate_cubes/s1_asf_clipped_cube.zarr\", mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
