{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5a73d7-6a11-40d8-9efe-43e1cc70e677",
   "metadata": {},
   "source": [
    "{{title_s1_1}}\n",
    "\n",
    "This notebook demonstrates working with Sentinel-1 RTC imagery that has been processed on the [ASF On-Demand server](https://docs.asf.alaska.edu/vertex/manual/) and downloaded locally. \n",
    "\n",
    "The downloaded time series of Sentinel-1 imagery is very large. We demonstrate strategies for reading data of this nature into memory by creating a *virtual copy* of the data. \n",
    "\n",
    ":::{important}\n",
    "As mentioned, the steps shown in this notebook involve downloading and extracting large volumes of data. **It is not necessary to do this to follow the rest of the content in the tutorial**. We include the demonstration for the purposes of completeness and to help users who may be in this situation.\n",
    "\n",
    "***To skip downloading the data and proceed with the tutorial, use the VRT files located in the `../tutorial2/data/` directory of the tutorial repository.***\n",
    ":::\n",
    "\n",
    "::::{tab-set}\n",
    ":::{tab-item} Outline   \n",
    "(content:section_A)=\n",
    "**[A. Prepare to read data into memory](#a-prepare-to-read-data-into-memory)**  \n",
    "- {{a1_s1_nb1}}  \n",
    "- {{a2_s1_nb1}}  \n",
    "\n",
    "(content:section_B)=\n",
    "**[B. Read data](#b-read-data)**  \n",
    "- {{b1_s1_nb1}}  \n",
    ":::\n",
    "\n",
    ":::{tab-item} Learning goals\n",
    "\n",
    "{{concepts}}\n",
    "- Understand local file storage and create virtual datasets from locally stored files\n",
    "- Read larger-than-memory data into memory\n",
    "- Use VRT objects to create xarray objects from large data stacks\n",
    ":::\n",
    "::::\n",
    ":::{admonition} ASF Data Access\n",
    "You can download the RTC-processed backscatter time series [here](https://zenodo.org/record/7236413#.Y1rNi37MJ-0). This tutorial starts from the point of having the data downloaded and unzipped in a directory. See the path to the directory location and the structure of the directory holding the unzipped files below. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52885f87",
   "metadata": {},
   "source": [
    "Expand the next cell to see specific packages used in this notebook and relevant system and version information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb32b85-47b7-4e1d-baeb-48af7116ce6e",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "%xmode minimal\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "import geopandas as gpd\n",
    "import markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import rioxarray as rio\n",
    "import xarray as xr\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e975ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "tutorial2_dir = pathlib.Path(cwd).parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdc79f",
   "metadata": {},
   "source": [
    "TODO Add section explaining how to download and unzip data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b2541",
   "metadata": {},
   "source": [
    "## A. Prepare to read data into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2fbfb",
   "metadata": {},
   "source": [
    "### {{a1_s1_nb1}}\n",
    "\n",
    "After the data is extracted from the compressed files, we have a directory containing sub-directories for each Sentinel-1 image acquisition (scene). Within each sub-directory are all of the files associated with that scene. For more information about the files contained in each directory, see this [section](https://hyp3-docs.asf.alaska.edu/guides/rtc_product_guide/#image-files) of the ASF Sentinel-1 RTC Product Guide.\n",
    "\n",
    "The directory should look like the diagram below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f33c79",
   "metadata": {},
   "source": [
    "```\n",
    ".\n",
    "└── s1_asf_data\n",
    "    ├── S1A_IW_20210502T121414_DVP_RTC30_G_gpuned_1424\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_VH.tif.xml\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_rgb.kmz\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_shape.prj\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7.png.aux.xml\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7.README.md.txt\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_rgb.png.aux.xml\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_rgb.png\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_VV.tif.xml\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7.png\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_rgb.png.xml\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7.png.xml\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_shape.shp\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_VH.tif\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7.log\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_VV.tif\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7.kmz\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_shape.dbf\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_ls_map.tif.xml\n",
    "    │   ├── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_shape.shx\n",
    "    │   └── S1A_IW_20220214T121353_DVP_RTC30_G_gpuned_51E7_ls_map.tif\n",
    "    └── S1A_IW_20210505T000307_DVP_RTC30_G_gpuned_54B1\n",
    "        └── ...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea76c0",
   "metadata": {},
   "source": [
    "To build GDAL VRT files, we need to pass a list of the input datasets. This means that we need to extract the file path to every file associated with each variable (VV, VH and L-S). \n",
    "\n",
    ":::{note}\n",
    "If you are following along on your own computer, be sure to replace the `s1_asf_data` file path below with the path to the location of the downloaded files on your own computer.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcf6dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to directory holding downloaded data\n",
    "s1_asf_data = pathlib.Path(cwd.parents[3], \"sentinel1_rtc/data/asf_rtcs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d641245",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_ls = os.listdir(s1_asf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff41701-8f7e-4f22-839f-47260a04419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract path for target files from each scene\n",
    "def extract_fnames(data_path: str, scene_name: str) -> list:\n",
    "    \"\"\"return a list of files associated with a single S1 scene\"\"\"\n",
    "    # Make list of files within each scene directory in data directory\n",
    "    scene_files_ls = os.listdir(os.path.join(data_path, scene_name))\n",
    "\n",
    "    # Make a list to hold README files\n",
    "    rm = [file for file in scene_files_ls if file.endswith(\"README.md.txt\")]\n",
    "\n",
    "    # Make a list to hold tif file names for each variable\n",
    "    scene_files_vv = [fname for fname in scene_files_ls if fname.endswith(\"_VV.tif\")]\n",
    "    scene_files_vh = [fname for fname in scene_files_ls if fname.endswith(\"_VH.tif\")]\n",
    "    scene_files_ls = [\n",
    "        fname for fname in scene_files_ls if fname.endswith(\"_ls_map.tif\")\n",
    "    ]\n",
    "\n",
    "    return scene_files_vv, scene_files_vh, scene_files_ls, rm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72424d",
   "metadata": {},
   "source": [
    "Below is the output of `extract_fnames()` for two sub-directories in the data directory. Note that `os.listdir()` **does not** preserve the order of the subdirectories as listed on disk. This is okay because we will ensure that the files are sorted in chronological order later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b409a588-e769-49d1-b62b-6f585dc9599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_fnames(s1_asf_data, scenes_ls[0]))\n",
    "print(extract_fnames(s1_asf_data, scenes_ls[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09499727",
   "metadata": {},
   "source": [
    "We need to attach the filenames to the path of each file so that we end up with a list of the full paths to the VV and VH band imagery, layover-shadow maps and README files. Within this step, we will also add checks to ensure that the process is doing what we want, which is to create lists of filepaths for each character **with the same order across lists** so that we can combine the lists into a multivariate time series. As noted above, it is okay that the lists are not in chronological order, as long as they are in **the same** order across variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b2361f-961e-463f-b9fa-66d094a24b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filename_lists(asf_s1_data_path: str):\n",
    "    # Make list of all scenes in dir\n",
    "    scenes_ls = os.listdir(asf_s1_data_path)\n",
    "\n",
    "    # Make empty lists to hold file paths for different variables\n",
    "    fpaths_vv, fpaths_vh, fpaths_ls, fpaths_rm = [], [], [], []\n",
    "\n",
    "    for element in range(len(scenes_ls)):\n",
    "        # Extract filenames of each file of interest\n",
    "        files_of_interest = extract_fnames(asf_s1_data_path, scenes_ls[element])\n",
    "\n",
    "        # Make full path with filename for each variable\n",
    "        path_vv = os.path.join(\n",
    "            asf_s1_data_path, scenes_ls[element], files_of_interest[0][0]\n",
    "        )\n",
    "        path_vh = os.path.join(\n",
    "            asf_s1_data_path, scenes_ls[element], files_of_interest[1][0]\n",
    "        )\n",
    "        path_ls = os.path.join(\n",
    "            asf_s1_data_path, scenes_ls[element], files_of_interest[2][0]\n",
    "        )\n",
    "        path_readme = os.path.join(\n",
    "            asf_s1_data_path, scenes_ls[element], files_of_interest[3][0]\n",
    "        )\n",
    "\n",
    "        # add a check to ensure that the files are aligned correctly\n",
    "        date_vv = pathlib.Path(path_vv).stem.split(\"_\")[2]\n",
    "        date_vh = pathlib.Path(path_vh).stem.split(\"_\")[2]\n",
    "        date_ls = pathlib.Path(path_ls).stem.split(\"_\")[2]\n",
    "        date_rm = pathlib.Path(path_readme).stem.split(\"_\")[2]\n",
    "        assert date_vh == date_vv == date_ls == date_rm, (\n",
    "            \"AssertionError: File dates do not match across variables.\"\n",
    "        )\n",
    "\n",
    "        fpaths_vv.append(path_vv)\n",
    "        fpaths_vh.append(path_vh)\n",
    "        fpaths_ls.append(path_ls)\n",
    "        fpaths_rm.append(path_readme)\n",
    "\n",
    "    # Check that all lists are the same length\n",
    "    assert len(fpaths_vv) == len(fpaths_vh) == len(fpaths_ls) == len(fpaths_rm), (\n",
    "        f\"Files weren't extracted correctly. Expected all lists to be the same length, received \\n\"\n",
    "        \"{len(fpaths_vv)}, {len(fpaths_vh)}, {len(fpaths_ls)}, {len(fpaths_rm)}\"\n",
    "    )\n",
    "    # Check that all lists are the same length\n",
    "    assert len(fpaths_vv) == len(fpaths_vh) == len(fpaths_ls) == len(fpaths_rm), (\n",
    "        \"Files weren't extracted correctly or fname lists weren't made correctly\"\n",
    "    )\n",
    "    return (fpaths_vv, fpaths_vh, fpaths_ls, fpaths_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c21d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths_vv, filepaths_vh, filepaths_ls, filepaths_rm = make_filename_lists(\n",
    "    s1_asf_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb54f366",
   "metadata": {},
   "source": [
    "Create a dictionary to hold the file paths for each variable so that we can use them more easily later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "239f96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths_dict = {\n",
    "    \"vv\": filepaths_vv,\n",
    "    \"vh\": filepaths_vh,\n",
    "    \"ls\": filepaths_ls,\n",
    "    \"readme\": filepaths_rm,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df335f-c7ca-4b0d-8051-c4882942ffbb",
   "metadata": {},
   "source": [
    "### {{a2_s1_nb1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98cd57-07c1-4e7d-ba68-ff20aecd074f",
   "metadata": {},
   "source": [
    "We will be using the `gdalbuildvrt` command. You can find out more about it [here](https://manpages.ubuntu.com/manpages/bionic/man1/gdalbuildvrt.1.html). This command creates a *virtual* GDAL dataset given a list of other GDAL datasets (the Sentinel-1 scenes). `gdalbuildvrt` can make a VRT that either tiles the listed files into a large spatial mosaic, or places them each in a separate band of the VRT. Because we are dealing with a temporal stack of images we want to use the `-separate` flag to place each file into a band of the VRT. </br>\n",
    "\n",
    "Here is where we use the list of file paths we created above. For each variable, write the list of file paths to a `.txt` file which is then passed to `gdalbuildvrt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9646960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vrt_object(fpaths_dict: dict, variable: str):\n",
    "    \"\"\"Function to create VRT files for each variable given a\n",
    "    list of file paths fo that variable.\"\"\"\n",
    "\n",
    "    # Write file paths to txt file\n",
    "    fpath_input_txt = f\"s1_{variable}_fpaths.txt\"\n",
    "\n",
    "    # Specify location of vrt file- note that we use\n",
    "    # tutorial2_dir path defined at top of notebook\n",
    "    # output_vrt_path = os.path.join(tutorial2_dir,\n",
    "    output_vrt_path = f\"../data/vrt_files/s1_stack_{variable}.vrt\"\n",
    "\n",
    "    # Write file paths to txt file\n",
    "    with open(fpath_input_txt, \"w\") as fp:\n",
    "        for item in fpaths_dict[f\"{variable}\"]:\n",
    "            fp.write(f\"{item}\\n\")\n",
    "\n",
    "    !gdalbuildvrt -separate -input_file_list {fpath_input_txt} {output_vrt_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_vrt_object(file_paths_dict, \"vv\")\n",
    "create_vrt_object(file_paths_dict, \"vh\")\n",
    "create_vrt_object(file_paths_dict, \"ls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff15f3-68ec-4842-b6ee-4551551fecf4",
   "metadata": {},
   "source": [
    "## B. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "359581f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv = xr.open_dataset(\"../data/vrt_files/s1_stackVV.vrt\", chunks=\"auto\")\n",
    "ds_vh = xr.open_dataset(\"../data/vrt_files/s1_stackVH.vrt\", chunks=\"auto\")\n",
    "ds_ls = xr.open_dataset(\"../data/vrt_files/s1_stackLS.vrt\", chunks=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb8e89-fcd3-4985-9b0e-3c652195969f",
   "metadata": {},
   "source": [
    "Building the `VRT` object assigns every object in the .txt file to a different band. In doing this, we lose the metadata that is associated with the files. The next notebook walks through the steps of reconstructing necessary metadata stored in file names and auxiliary files and attaching it to the Xarray objects read from VRTs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979db2a7",
   "metadata": {},
   "source": [
    "### {{b1_s1_nb1}}\n",
    "\n",
    "```{image} ../imgs/s1_chunks.png\n",
    ":alt chunks_image\n",
    ":align_center\n",
    "```\n",
    "\n",
    "Each variable in `ds` has a total shape of (103, 13379, 1742) and is 89.59 GB. It is chunked so that each chunk is (11, 1536, 1536) and 99 MB, with 1080 total chunks per variable. \n",
    "\n",
    "Depending on your use-case, you may want to adjust the chunking of the object. For example, if you are interested in analyzing variability along the temporal dimension, it might make sense to re-chunk the dataset so that operations along the that dimension are more easily parallelized. For more detail, see the {term}`chunking` discussion in [Relevant Concepts](../../background/relevant_concepts.md) and the [Parallel Computing with Dask](https://docs.xarray.dev/en/stable/user-guide/dask.html) section of the Xarray documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2ad54",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated reading large data into memory by creating a virtual dataset that references that full dataset without directly reading it. \n",
    "\n",
    "However, we also saw that reading the data in this way produces an object that lacks important metadata. The next notebook will go through the steps of locating and adding relevant metadata to the backscatter data cubes read in this notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
