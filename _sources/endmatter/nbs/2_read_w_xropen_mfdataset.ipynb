{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626f0868-f402-4377-b50d-1eb7c4a4b2da",
   "metadata": {},
   "source": [
    "# Read Sentinel-1 RTC data processed by ASF using `xr.open_mfdataset()`\n",
    "\n",
    "This notebook demonstrates working with Sentinel-1 RTC imagery that has been processed on the ASF On-Demand server and downloaded locally. \n",
    "\n",
    "The access point for data in this notebook is a directory containing un-zipped directories of RTC scenes.\n",
    "\n",
    "This notebook will detail how to use the xarray function `xr.open_mfdataset()`. While this approach works and is very useful for working with large stacks of data with associated metadata, you will also see an example of the limitations of this approach due to certain characteristics of the dataset. You can read more about `xr.open_mfdataset()` [here](https://docs.xarray.dev/en/stable/generated/xarray.open_mfdataset.html).\n",
    "\n",
    "**Learning goals**:\n",
    "\n",
    "- Organize large set of geotiff files stored locally\n",
    "- Read data into memory using Xarray's `open_mfdataset()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cd65f-4a6b-4f66-98ba-8b705a7e642a",
   "metadata": {},
   "source": [
    "## Software and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb32b85-47b7-4e1d-baeb-48af7116ce6e",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "from functools import partial\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import re\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e581e-2305-4b1c-865e-75d7cea30f35",
   "metadata": {},
   "source": [
    "Initialize a `dask.distributed` client:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbbb86-41d9-4e71-9ecc-3cfbc35d01ff",
   "metadata": {},
   "source": [
    "```{note} \n",
    "On my local machine, I ran into issues initially when the cluster was running on processes rather than threads. This caused system memory issues and many dead kernels. Setting `processes=False` seemed to fix these issues\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7c0ee-eb3d-42a4-bb00-c4cb8b11c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(processes=False)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a906cc1-1d1e-4d6e-9775-46dfdc6c6ad5",
   "metadata": {},
   "source": [
    "Open the dask dashboard at the link above to monitor task progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48327c7f-982a-46e7-b1f9-39d527d66b8f",
   "metadata": {},
   "source": [
    "### Organize file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d128b-1696-4eba-a8a6-b89b08ea2764",
   "metadata": {},
   "source": [
    "Set up some string variables for directory paths. We need to pass `xr.open_mfdataset()` a list of all files to read in. Currently, the file structure is organized so that each scene has its own sub-directory within `asf_rtcs`. Within each sub-directory are several files - we want to extract the GeoTIFF files containing RTC imagery for the VV and VH polarizations for each scene as well as the layover-shadow mask for each scene. The function `extract_fnames()` takes a path to the directory containing the sub-directories for all scenes and returns a list of the filenames for VV-polarization GeoTIFF files, VH-polarization GeoTIFF files and layover-shadow mask GeoTIFF files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "tutorial2_dir = pathlib.Path(cwd).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_type = \"full\"\n",
    "path_to_rtcs = f\"sentinel1/data/{timeseries_type}_timeseries/asf_rtcs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to directory holding downloaded data\n",
    "s1_asf_data = os.path.join(str(cwd.parents[1]), path_to_rtcs)\n",
    "scenes_ls = os.listdir(s1_asf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fnames(data_path: str, scene_name: str, variable: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract filenames for a specific variable from a given scene directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to the directory containing scene sub-directories.\n",
    "    scene_name : str\n",
    "        Name of the scene sub-directory.\n",
    "    variable : str\n",
    "        Variable type to extract filenames for. Options are 'vv', 'vh', 'ls_map', or 'readme'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of filenames matching the specified variable within the scene directory.\n",
    "    \"\"\"\n",
    "    # Make list of files within each scene directory in data directory\n",
    "    scene_files_ls = os.listdir(os.path.join(data_path, scene_name))\n",
    "\n",
    "    if variable in [\"vv\", \"vh\"]:\n",
    "        scene_files = [fname for fname in scene_files_ls if fname.endswith(f\"_{variable.upper()}.tif\")]\n",
    "\n",
    "    elif variable == \"ls_map\":\n",
    "        scene_files = [fname for fname in scene_files_ls if fname.endswith(\"_ls_map.tif\")]\n",
    "\n",
    "    elif variable == \"readme\":\n",
    "        scene_files = [file for file in scene_files_ls if file.endswith(\"README.md.txt\")]\n",
    "\n",
    "    return scene_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_fnames(s1_asf_data, scenes_ls[0], \"vv\"))\n",
    "print(extract_fnames(s1_asf_data, scenes_ls[0], \"vh\"))\n",
    "print(extract_fnames(s1_asf_data, scenes_ls[0], \"ls_map\"))\n",
    "print(extract_fnames(s1_asf_data, scenes_ls[0], \"readme\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff41701-8f7e-4f22-839f-47260a04419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filepath_lists(asf_s1_data_path: str, variable: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Generate lists of file paths and acquisition dates for a specific variable from Sentinel-1 RTC data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    asf_s1_data_path : str\n",
    "        Path to the directory containing scene sub-directories.\n",
    "    variable : str\n",
    "        Variable type to extract filenames for. Options are 'vv', 'vh', 'ls_map', or 'readme'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing two lists:\n",
    "        - fpaths (list of str): List of full file paths for the specified variable.\n",
    "        - dates_ls (list of str): List of acquisition dates corresponding to each file path.\n",
    "    \"\"\"\n",
    "    scenes_ls = os.listdir(asf_s1_data_path)\n",
    "\n",
    "    fpaths, dates_ls = [], []\n",
    "\n",
    "    for element in range(len(scenes_ls)):\n",
    "        # Extract filenames of each file of interest\n",
    "        files_of_interest = extract_fnames(asf_s1_data_path, scenes_ls[element], variable)\n",
    "        # Make full path with filename for each variable\n",
    "        path = os.path.join(asf_s1_data_path, scenes_ls[element], files_of_interest[0])\n",
    "        # Extract dates to make sure dates are identical across variable lists\n",
    "        date = pathlib.Path(path).stem.split(\"_\")[2]\n",
    "\n",
    "        dates_ls.append(date)\n",
    "        fpaths.append(path)\n",
    "\n",
    "    return (fpaths, dates_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895eb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filenames_dict(rtc_path: str, variables_ls: list) -> dict:\n",
    "    \"\"\"\n",
    "    Create a dictionary of filenames for specified variables from Sentinel-1 RTC data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rtc_path : str\n",
    "        Path to the directory containing scene sub-directories.\n",
    "    variables_ls : list\n",
    "        List of variable types to extract filenames for. Options are 'vv', 'vh', 'ls_map', or 'readme'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where keys are variable names and values are lists of file paths for the specified variables.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        If the dates are not identical across all lists or if the length of each variable list is not the same.\n",
    "    \"\"\"\n",
    "    keys, filepaths, dates = [], [], []\n",
    "    for variable in variables_ls:\n",
    "        keys.append(variable)\n",
    "\n",
    "        filespaths_list, dates_list = make_filepath_lists(rtc_path, variable)\n",
    "        filepaths.append(filespaths_list)\n",
    "        dates.append(dates_list)\n",
    "\n",
    "    # Make dict of variable names (keys) and associated filepaths\n",
    "    filepaths_dict = dict(zip(keys, filepaths))\n",
    "\n",
    "    # Make sure that dates are identical across all lists\n",
    "    assert all(lst == dates[0] for lst in dates) == True\n",
    "    # Make sure length of each variable list is the same\n",
    "    assert len(list(set([len(v) for k, v in filepaths_dict.items()]))) == 1\n",
    "\n",
    "    return filepaths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ce344",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths_dict = create_filenames_dict(s1_asf_data, [\"vv\", \"vh\", \"ls_map\", \"readme\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb98694",
   "metadata": {},
   "source": [
    "`filepaths_dict` has a key for each file type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f0deba-fecc-491b-b52c-f1748e432fd8",
   "metadata": {},
   "source": [
    "## Read files using `xr.open_mfdataset()`\n",
    "\n",
    "The `xr.open_mfdataset()` function reads multiple files (in a directory or from a list) and combines them to return a single `xr.Dataset` or `xr.DataArray`. To use the function, specify parameters such as how the files should be combined as well as any preprocessing to execute on the original files. This example will demonstrate a workflow for using `open_mfdataset()` to read in three stacks of roughly 100 RTC images.\n",
    "\n",
    "Here is an example of reading a single file with `xr.open_datatset()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc2220-8d5d-47d2-b5c8-c9f49f0b6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = xr.open_dataset(filepaths_dict[\"vv\"][0])\n",
    "ds1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22225aef",
   "metadata": {},
   "source": [
    "We're missing a lot of important metadata such as the type of data (VV, VH or ls_map) and the time associated with the observation. This is stored in the filename. We will use a preprocess function to tell Xarray how to parse the filename and organize the related metadata for each file in the list passed to `xr.open_mfdataset()`. The preprocess function will use `parse_fname_metadata()`, defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fname_metadata(input_fname: str) -> dict:\n",
    "    \"\"\"Function to extract information from filename and separate into expected variables based on a defined schema.\"\"\"\n",
    "    # Define schema\n",
    "    schema = {\n",
    "        \"sensor\": (3, r\"S1[A-B]\"),  # schema for sensor\n",
    "        \"beam_mode\": (2, r\"[A-Z]{2}\"),  # schema for beam mode\n",
    "        \"acq_date\": (15, r\"[0-9]{8}T[0-9]{6}\"),  # schema for acquisition date\n",
    "        \"pol_orbit\": (3, r\"[A-Z]{3}\"),  # schema for polarization + orbit type\n",
    "        \"terrain_correction_pixel_spacing\": (\n",
    "            5,\n",
    "            r\"RTC[0-9]{2}\",\n",
    "        ),  # schema for terrain correction pixel spacing\n",
    "        \"processing_software\": (\n",
    "            1,\n",
    "            r\"[A-Z]{1}\",\n",
    "        ),  # schema for processing software (G = Gamma)\n",
    "        \"output_info\": (6, r\"[a-z]{6}\"),  # schema for output info\n",
    "        \"product_id\": (4, r\"[A-Z0-9]{4}\"),  # schema for product id\n",
    "        \"prod_type\": ((2, 6), (r\"[A-Z]{2}\", r\"ls_map\")),  # schema for polarization type\n",
    "    }\n",
    "\n",
    "    # Remove prefixs\n",
    "    input_fname = input_fname.split(\"/\")[-1]\n",
    "    # Remove file extension if present\n",
    "    input_fname = input_fname.removesuffix(\".tif\")\n",
    "    # Split filename string into parts\n",
    "    parts = input_fname.split(\"_\")\n",
    "\n",
    "    # l-s map objects have an extra '_' in the filename. Remove/combine parts so that it matches schema\n",
    "    if parts[-1] == \"map\":\n",
    "        parts = parts[:-1]\n",
    "        parts[-1] = parts[-1] + \"_map\"\n",
    "\n",
    "    # Check that number of parts matches expected schema\n",
    "    if len(parts) != len(schema):\n",
    "        raise ValueError(f\"Input filename does not match schema of expected format: {parts}\")\n",
    "\n",
    "    # Create dict to store parsed data\n",
    "    parsed_data = {}\n",
    "\n",
    "    # Iterate through parts and schema\n",
    "    for part, (name, (length_options, pattern_options)) in zip(parts, schema.items()):\n",
    "        # In the schema we defined, items have an int for length or a tuple (when there is more than one possible lenght)\n",
    "        # Make the int lengths into tuples\n",
    "        if isinstance(length_options, int):\n",
    "            length_options = (length_options,)\n",
    "        # Same as above for patterns\n",
    "        if isinstance(pattern_options, str):\n",
    "            pattern_options = (pattern_options,)\n",
    "\n",
    "        # Check that each length of each part matches expected length from schema\n",
    "        if len(part) not in length_options:\n",
    "            raise ValueError(f\"Part {part} does not have expected length {len(part)}\")\n",
    "        # Check that each part matches expected pattern from schema\n",
    "        if not any(re.fullmatch(pattern, part) for pattern in pattern_options):\n",
    "            raise ValueError(f\"Part {part} does not match expected patterns {pattern_options}\")\n",
    "\n",
    "        # Special handling of a part (pol orbit) that has 3 types of metadata\n",
    "        if name == \"pol_orbit\":\n",
    "            parsed_data.update(\n",
    "                {\n",
    "                    \"polarization_type\": part[:1],  # Single (S) or Dual (D) pol\n",
    "                    \"primary_polarization\": part[1:2],  # Primary polarization (H or V)\n",
    "                    \"orbit_type\": part[-1],  # Precise (p), Restituted (r) or Original predicted (o)\n",
    "                }\n",
    "            )\n",
    "        # Format string acquisition date as a datetime time stamp\n",
    "        elif name == \"acq_date\":\n",
    "            parsed_data[name] = pd.to_datetime(part, format=\"%Y%m%dT%H%M%S\")\n",
    "        # Expand multiple variables stored in output_info string part\n",
    "        elif name == \"output_info\":\n",
    "            output_info_keys = [\n",
    "                \"output_type\",\n",
    "                \"output_unit\",\n",
    "                \"unmasked_or_watermasked\",\n",
    "                \"notfiltered_or_filtered\",\n",
    "                \"area_or_clipped\",\n",
    "                \"deadreckoning_or_demmatch\",\n",
    "            ]\n",
    "\n",
    "            output_info_values = [part[0], part[1], part[2], part[3], part[4], part[-1]]\n",
    "\n",
    "            parsed_data.update(dict(zip(output_info_keys, output_info_values)))\n",
    "\n",
    "        else:\n",
    "            parsed_data[name] = part\n",
    "\n",
    "    # Because we have already addressed product type in the variable names\n",
    "    prod_type = parsed_data.pop(\"prod_type\")\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ff208",
   "metadata": {},
   "source": [
    "Now that we've written a function to create a dictionary of relevant metadata extracted from a filename, we need to use it within a preprocess function. Preprocess functions ingest and return `xr.Dataset` or `xr.DataArray` objects; in other words, they act on each unit passed to `open_mfdataset()`. Your preprocess function should contain all of the steps that must be applied to a single file/`xr.Dataset` object to end up with an analysis-ready cube. The `preprocess` function below:\n",
    "- Renames the data variable from `'band_data'` to a descriptive name,\n",
    "- Extracts the file name associated with the `xr.Dataset` (from the object's encoding),\n",
    "- Parses information stored in the filename and adds it as an `attrs` dictionary,\n",
    "- Extracts coordinate reference system metadata,\n",
    "- Reshapes the dataset into a 3-dimensional `x,y,time` cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds_orig: xr.DataArray, variable: str):\n",
    "    \"\"\"function that should return an xarray object with time dimension and associated metadata given a path to a single RTC scene, if its dualpol will have multiple bands, currently just as 2 data arrays but could merge.\n",
    "    goal would be to apply this a list of directories for different RTC products, return cube along time dimension - I think?\n",
    "    - for concatenating, would need to check footprints and only take products with the same footprint, or subset them all to a common AOI?\n",
    "    \"\"\"\n",
    "\n",
    "    # fname_ls = []\n",
    "\n",
    "    ds = ds_orig.copy()\n",
    "    ds = ds.rename({\"band_data\": variable}).squeeze()\n",
    "\n",
    "    # vv_fn = da_orig.encoding['source'][113:]\n",
    "    vh_fn = os.path.basename(ds_orig[\"band_data\"].encoding[\"source\"])\n",
    "    # print('fname: ', vv_fn)\n",
    "\n",
    "    attrs_dict = parse_fname_metadata(vh_fn)\n",
    "\n",
    "    # link the strings for each of the above variables to their full names (from README, commented above)\n",
    "    # eg if output_type=g, should read 'gamma'\n",
    "\n",
    "    ds.attrs = attrs_dict\n",
    "\n",
    "    utm_zone = ds.spatial_ref.attrs[\"crs_wkt\"][17:29]\n",
    "    epsg_code = ds.spatial_ref.attrs[\"crs_wkt\"][589:594]\n",
    "\n",
    "    ds.attrs[\"utm_zone\"] = utm_zone\n",
    "    ds.attrs[\"epsg_code\"] = f\"EPSG:{epsg_code}\"\n",
    "\n",
    "    date = ds.attrs[\"acq_date\"]\n",
    "\n",
    "    ds = ds.assign_coords({\"acq_date\": date})\n",
    "    ds = ds.expand_dims(\"acq_date\")\n",
    "    ds = ds.drop_duplicates(dim=[\"x\", \"y\"])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86cf980",
   "metadata": {},
   "source": [
    "Create the wrapper function for each variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_vh_partial = partial(preprocess, variable=\"vh\")\n",
    "preprocess_vv_partial = partial(preprocess, variable=\"vv\")\n",
    "preprocess_ls_partial = partial(preprocess, variable=\"ls_map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6e8db",
   "metadata": {},
   "source": [
    "### An example of complicated chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c022c13-c49f-4b56-9463-49843294c934",
   "metadata": {},
   "source": [
    "First, let's call `xr.open_mfdataset()` with the argument `chunks='auto'`. This will read in a dask array where ideal chunk sizes are selected based off the array size, it will attempt to have chunk sizes where bytes are equal to the configuration value for array chunk size. More about that [here](https://docs.dask.org/en/stable/array-chunks.html#automatic-chunking). You can check the configuration value for an array chunk size with the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b64b1-485c-4544-89dc-c96a4f64300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.get(\"array.chunk-size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d1791-174f-4907-8a4a-085fc24fb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_vh = xr.open_mfdataset(\n",
    "    paths=filepaths_dict[\"vh\"],\n",
    "    preprocess=preprocess_vh_partial,\n",
    "    chunks=\"auto\",\n",
    "    engine=\"rasterio\",\n",
    "    data_vars=\"minimal\",\n",
    "    coords=\"minimal\",\n",
    "    concat_dim=\"acq_date\",\n",
    "    combine=\"nested\",\n",
    "    parallel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_vv = xr.open_mfdataset(\n",
    "    paths=filepaths_dict[\"vv\"],\n",
    "    preprocess=preprocess_vv_partial,\n",
    "    chunks=\"auto\",\n",
    "    engine=\"rasterio\",\n",
    "    data_vars=\"minimal\",\n",
    "    coords=\"minimal\",\n",
    "    concat_dim=\"acq_date\",\n",
    "    combine=\"nested\",\n",
    "    parallel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_ls = xr.open_mfdataset(\n",
    "    paths=filepaths_dict[\"ls_map\"],\n",
    "    preprocess=preprocess_ls_partial,\n",
    "    chunks=\"auto\",\n",
    "    engine=\"rasterio\",\n",
    "    data_vars=\"minimal\",\n",
    "    coords=\"minimal\",\n",
    "    concat_dim=\"acq_date\",\n",
    "    combine=\"nested\",\n",
    "    parallel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1eb1c-04ee-41fb-ba78-e2e30621cf58",
   "metadata": {},
   "source": [
    "If we take a look at the `asf_vh` object we just created (click the stack icon on the right of the `vh` variable tab), we see that the chunking along the x and y dimensions is quite complicated. This isn't ideal because it can create problems like excessive communication between workers (and a lot of memory usage) down the line when we perform non-lazy operations. It seems like the `'auto'` chunking is applied to the top layer (file) of the dataset, but because the spatial footprint of each file is not the same, the 'auto' chunking does not persist through layers and we end up with the funky layout in the object above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a8b8b",
   "metadata": {},
   "source": [
    "Ensure that each of the three objects have the same coordinate dimensions so that they can be combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbded1-a7a7-4774-9eed-2152274efe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert asf_vh.dims == asf_vv.dims == asf_ls.dims\n",
    "\n",
    "assert asf_ls.acq_date.equals(asf_vv.acq_date)\n",
    "assert asf_ls.acq_date.equals(asf_vh.acq_date)\n",
    "\n",
    "assert asf_vh.x.equals(asf_vv.x)\n",
    "assert asf_vh.x.equals(asf_ls.x)\n",
    "\n",
    "assert asf_vh.y.equals(asf_vv.y)\n",
    "assert asf_vh.y.equals(asf_ls.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ee5f4-eac3-4c28-a978-77d2674e849d",
   "metadata": {},
   "source": [
    "Merge the VH, VV and layover-shadow mask objects into one `xr.Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8312220-60f4-4f99-941d-aea8644b8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_ds = xr.Dataset({\"vv\": asf_vv.vv, \"vh\": asf_vh.vh, \"ls\": asf_ls.ls_map})\n",
    "asf_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0814c-5996-44d0-86c3-5185fed0cc0a",
   "metadata": {},
   "source": [
    "If you take a look at the `acq_date` coordinate, you will see that they are not in order. Let's sort by the `acq_date` (time) dimension using xarray `.sortby()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1ae58-8098-40a9-b1a5-16dfc2d95d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_ds_sorted = asf_ds.sortby(asf_ds.acq_date)\n",
    "asf_ds_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2f603",
   "metadata": {},
   "source": [
    "How large is this object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24390771",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_ds_sorted.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9c83a-4587-489b-b42a-468286d97da8",
   "metadata": {},
   "source": [
    "## Clip stack to AOI using `rioxarray.clip()`\n",
    "\n",
    "This is a pretty unwieldy object (nearly 300 GB). Let's subset it down to just the area we want to focus on. Read in the following GeoJSON to clip the dataset to the area of interest we'll be using in this tutorial. This AOI covers a region in the central Himalaya near the Chinese border. We will use the `rioxarray.clip()` function which you can read more about [here](https://corteva.github.io/rioxarray/stable/examples/clip_geom.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac043141-d3f8-4ac4-a68a-b79f0c8e9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_aoi = gpd.read_file(\"https://github.com/e-marshall/sentinel1_rtc/raw/main/hma_rtc_aoi.geojson\")\n",
    "pc_aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b9d57-defe-4d31-9ce5-3f8436de581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_aoi.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfffb6c",
   "metadata": {},
   "source": [
    "Clip the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c471d-00a6-4b15-8d0e-73d02d6d2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_clip = asf_ds_sorted.rio.clip(pc_aoi.geometry, pc_aoi.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe9249-b6c4-4c47-b159-476196dfdb10",
   "metadata": {},
   "source": [
    "Check the size of the clipped object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbd96d-a06a-42f2-9f82-377f99ec9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_clip.nbytes / 1e9  # units are GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca3b5a-7c1a-4b4a-97fc-22c8dd7bd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't want to compute this right now, will kill kernel\n",
    "# asf_clip_load = asf_clip.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8dd4f-dd2a-4048-82db-4271057a9d13",
   "metadata": {},
   "source": [
    "The dataset is published as gamma-nought values on the power scale. This is useful for performing statistical analysis but not always for visualization. We convert to decibel scale (sigma-nought) in order to visualize variability in backscatter more easily. Read more about scales used to represent SAR data [here](https://hyp3-docs.asf.alaska.edu/guides/rtc_product_guide/#sar-scales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f982c-a421-4423-a70b-18dcadc46a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_to_db(input_arr):\n",
    "    return 10 * np.log10(np.abs(input_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40daae69-def7-4e5d-a0be-0c73eda778d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(18, 7))\n",
    "\n",
    "asf_clip.isel(acq_date=10).ls.plot(ax=axs[0])\n",
    "power_to_db(asf_clip.isel(acq_date=10).vv).plot(ax=axs[1], cmap=plt.cm.Greys_r)\n",
    "power_to_db(asf_clip.isel(acq_date=10).vh).plot(ax=axs[2], cmap=plt.cm.Greys_r)\n",
    "fig.suptitle(\n",
    "    f\"Layover-shadow mask (L), VV (C) and VH (R) backscatter on {str(asf_clip.isel(acq_date=10).acq_date.data)[:-19]}\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581daf8-51fd-4164-838e-c39b0a0ca590",
   "metadata": {},
   "source": [
    "## Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e55abe-8a2d-433d-9619-26bb8ea1d2f9",
   "metadata": {},
   "source": [
    "The `xr.open_mfdataset()` approach is very useful for reading in large amounts of data from a number of distributed files very efficiently. It's especially helpful that the `preprocess()` function allows a large degree of customization for how the data is read in, and that the appropriate metadata is preserved as individual `xr.DataArrays` are organized into data cubes and we have to do very little further work of organizing metadata. \n",
    "\n",
    "However, due to the nature of this stack of data, where each time-element of the stack covers a common region of interest but does not have a uniform spatial footprint, this approach is not very computationally efficient. We are reading in a vast footprint of data to subset it to a much smaller area. While it is able to execute on this machine, a more computationally-intensive workflow would fail. \n",
    "\n",
    "Another approach we can try is to use GDAL VRT objects. GDAL VRT objects create an xml file from a list of GeoTIFF files. The xml contains a mapping of all of the specified raster files so that we essentially have the spatial information that would have been used to create the full data object, and all of the information we need for a clip. VRT objects are able to handle the mismatch of grids within the stack of files, so does not encounter the memory issues that we get when trying the `xr.open_mfdataset()` approach with dask. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
