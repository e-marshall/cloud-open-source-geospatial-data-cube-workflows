{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Wrangle metadata\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the last notebook, we demonstrated reading a larger-than-memory dataset using GDAL virtual datasets. Building the GDAL virtual dataset combines the stack of images by assigning each image to a `'band'` dimension. However, reading the data in this way caused important metadata to be lost. This notebook details steps of parsing metadata stored in filenames and separate files and formatting it so that it can be combined with the `Xr.Dataset` holding Sentinel-1 backscatter imagery, the physical observable we're interested in. \n",
    "\n",
    "A good first step here is to check out the README file that is located within each scene directory. This has important metadata that we need as well as instructions for how to interpret the file names. Check that out now if you haven't yet. \n",
    "\n",
    "We need to bring the information from the README files and the file names and organize it so that it is most useful to us as we try to interpret the Sentinel-1 backscatter imagery. \n",
    "\n",
    ":::{admonition} ASF data access options\n",
    "The steps shown in this notebook involve downloading and extracting large volumes of data. **It is not necessary to do this to follow the rest of the content in the tutorial**. We include the demonstration for the purposes of completeness and to help users who may be in this situation.\n",
    "\n",
    "For more information on different options for downloading data locally, see the [Introduction](../s1_intro.md#different-ways-to-use-this-tutorial).\n",
    ":::\n",
    "\n",
    "\n",
    "::::{tab-set}\n",
    ":::{tab-item} Outline  \n",
    "(content.Section_A)=\n",
    "**[A. Read and inspect initial metadata](#a-read-data-and-inspect-initial-metadata)**\n",
    "\n",
    "- 1) Add appropriate names to variables \n",
    "- 2) What metadata currently exists?\n",
    "\n",
    "(content.Section_B)=\n",
    "**[B. Add metadata from file name](#b-add-metadata-from-file-name)**  \n",
    "\n",
    "- 1) Parse file name\n",
    "- 2) Extract and format acquisition dates \n",
    "- 3) Combine data cubes\n",
    "\n",
    "(content.Section_C)=\n",
    "**[C. Time-varying metadata](#c-time-varying-metadata)** \n",
    "\n",
    "- 1) Extract attributes as lists of dictionaries   \n",
    "- 2) Create tuple of metadata for each type of information\n",
    "- 3) Assign metadata tuple to Xarray dataset as a coordinate variable\n",
    "- 4) Combine `xr.Dataset` of backscatter data with `xr.Dataset` of coordinate data\n",
    "\n",
    "(content.Section_D)=\n",
    "**[D. Add metadata from README file](#d-add-metadata-from-readme-file)**  \n",
    "\n",
    "- 1) Extract granule ID\n",
    "- 2) Build coordinate `xr.DataArray`\n",
    "\n",
    "(content.Section_E)=\n",
    "**[E. 3-d metadata](#e-3-d-metadata)**\n",
    "- 1) Set data variable containing metadata as coordinate variable\n",
    "\n",
    "\n",
    ":::\n",
    ":::{tab-item} Learning goals\n",
    "\n",
    "#### Concepts\n",
    "- Understand the connection between metadata and physical observable data.  \n",
    "- Format information within a data cube so that metadata maximizes usability of data cube.  \n",
    "- Understand processing attributes of Sentinel-1 imagery and file naming conventions.  \n",
    "\n",
    "#### Techniques\n",
    "- Parse strings in file names and markdown files using schema with [regex](https://docs.python.org/3/library/re.html) expressions to ensure expected behavior.  \n",
    "- Use Xarray coordinates and attributes to store different types of metadata.  \n",
    "- Combine multiple raster data cubes with [`xr.combine_by_coords()`](https://docs.xarray.dev/en/latest/generated/xarray.combine_by_coords.html).  \n",
    "\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext rich\n",
    "%xmode minimal\n",
    "\n",
    "import cf_xarray\n",
    "import markdown\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import re\n",
    "import xarray as xr\n",
    "\n",
    "import s1_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Read data and inspect initial metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{attention} \n",
    "If you are following along on your own computer, be sure to specify two things:\n",
    "1. Set `path_to_rtcs` to the location of the downloaded files on your own computer.\n",
    "2. Set `timeseries_type` to `'full'` or `'subset'` depending on if you are using the full time series (103 files) or the subset time series (5 files).\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set time series type\n",
    "timeseries_type = \"full\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "The default location of the data downloaded in this tutorial (in the [data download notebook](download_zenodo_data_curl.ipynb) is:  \n",
    "`../book/sentinel1/data/raster_data/{timeseries_type}_timeseries/'`.   \n",
    "Here, `{timeseries_type} is a stand-in for 'full' or 'subset', depending on which data you're using. \n",
    "\n",
    "If you prefer to store the data in a different location on your computer, update the `path_to_rtcs` variable below to be the full path of the downloaded data. The path should end with 'asf_rtcs.' For example, if you plan to store the data for this tutorial in your Downloads directory, you should specify `path_to_rtcs` as:   \n",
    "```python\n",
    "path_to_rtcs = \"/home/User/Downloads/asf_rtcs\"\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_rtcs = f\"../data/raster_data/{timeseries_type}_timeseries/asf_rtcs\"\n",
    "path_to_rtcs = \"/home/emmamarshall/Desktop/book_data/asf_rtcs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the previous notebook, define a variable representing the path to the parent dir of asf_rtcs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = str(pathlib.Path(path_to_rtcs).parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the VRT files using `xr.open_dataset()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv = xr.open_dataset(\n",
    "    f'{data_path}/vrt_files/s1_stack_vv.vrt', chunks=\"auto\",)\n",
    "ds_vh = xr.open_dataset(\n",
    "    f'{data_path}/vrt_files/s1_stack_vh.vrt', chunks=\"auto\",)\n",
    "ds_ls = xr.open_dataset(\n",
    "    f'{data_path}/vrt_files/s1_stack_ls_map.vrt', chunks=\"auto\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Add appropriate names to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv = ds_vv.rename({\"band_data\": \"vv\"})\n",
    "ds_vh = ds_vh.rename({\"band_data\": \"vh\"})\n",
    "ds_ls = ds_ls.rename({\"band_data\": \"ls\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) What metadata currently exists? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `cf_xarray` to parse the metadata contained in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv.cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we're lacking a lot of information, but we do have spatial reference information that allows us to understand how the data cube's pixels correspond to locations on Earth's surface (we also know this from the README file and the shp file in each scene's directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `x` and `y` dimensions have associated coordinate variables and there is a `'spatial_ref'` variable that gives coordinate reference system information about those coordinates. In the attributes of `'spatial_ref'`, we see that our object is projected in local UTM (Universal Transverse Mercator) zone 45N. This tells us that the units of the coordinate arrays are meters. Read more about UTM zones [here](https://www.usgs.gov/faqs/what-does-term-utm-mean-utm-better-or-more-accurate-latitudelongitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv[\"spatial_ref\"].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we know where our data is located in space. But, with the data in its current form, we don't know much else about how or when it was collected. The rest of this notebook will demonstrate finding that information from both the file names of the individual files and the other files located in the scene directories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Add metadata from file name \n",
    "\n",
    "Perhaps most importantly, we need to know when these observations were collected in time.\n",
    "\n",
    "As we know from the README, information about acquisitions dates and more is stored in the directory name for each scene as well as the individual file names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vh.band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Parse file name\n",
    "The function below extracts metadata from the filename of each GeoTIFF file in the time series. It uses [regex expressions](https://docs.python.org/3/howto/regex.html) to parse the different types of information stored within the file name based on the instructions from the README. Once parsed, the information is stored as a dictionary where each key is the type of information (ie. Acquisition start date and time, polarization, imaging mode, etc.) and each value is the corresponding information from the file name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fname_metadata(input_fname: str) -> dict:\n",
    "    \"\"\"Function to extract information from filename and separate into expected variables based on a defined schema.\"\"\"\n",
    "    # Define schema\n",
    "    schema = {\n",
    "        \"sensor\": (3, r\"S1[A-B]\"),  # schema for sensor\n",
    "        \"beam_mode\": (2, r\"[A-Z]{2}\"),  # schema for beam mode\n",
    "        \"acq_date\": (15, r\"[0-9]{8}T[0-9]{6}\"),  # schema for acquisition date\n",
    "        \"pol_orbit\": (3, r\"[A-Z]{3}\"),  # schema for polarization + orbit type\n",
    "        \"terrain_correction_pixel_spacing\": (\n",
    "            5,\n",
    "            r\"RTC[0-9]{2}\",\n",
    "        ),  # schema for terrain correction pixel spacing\n",
    "        \"processing_software\": (\n",
    "            1,\n",
    "            r\"[A-Z]{1}\",\n",
    "        ),  # schema for processing software (G = Gamma)\n",
    "        \"output_info\": (6, r\"[a-z]{6}\"),  # schema for output info\n",
    "        \"product_id\": (4, r\"[A-Z0-9]{4}\"),  # schema for product id\n",
    "        \"prod_type\": ((2, 6), (r\"[A-Z]{2}\", r\"ls_map\")),  # schema for polarization type\n",
    "    }\n",
    "\n",
    "    # Remove prefixs\n",
    "    input_fname = input_fname.split(\"/\")[-1]\n",
    "    # Remove file extension if present\n",
    "    input_fname = input_fname.removesuffix(\".tif\")\n",
    "    # Split filename string into parts\n",
    "    parts = input_fname.split(\"_\")\n",
    "\n",
    "    # l-s map objects have an extra '_' in the filename. Remove/combine parts so that it matches schema\n",
    "    if parts[-1] == \"map\":\n",
    "        parts = parts[:-1]\n",
    "        parts[-1] = parts[-1] + \"_map\"\n",
    "\n",
    "    # Check that number of parts matches expected schema\n",
    "    if len(parts) != len(schema):\n",
    "        raise ValueError(f\"Input filename does not match schema of expected format: {parts}\")\n",
    "\n",
    "    # Create dict to store parsed data\n",
    "    parsed_data = {}\n",
    "\n",
    "    # Iterate through parts and schema\n",
    "    for part, (name, (length_options, pattern_options)) in zip(parts, schema.items()):\n",
    "        # In the schema we defined, items have an int for length or a tuple (when there is more than one possible length)\n",
    "        # Make the int lengths into tuples\n",
    "        if isinstance(length_options, int):\n",
    "            length_options = (length_options,)\n",
    "        # Same as above for patterns\n",
    "        if isinstance(pattern_options, str):\n",
    "            pattern_options = (pattern_options,)\n",
    "\n",
    "        # Check that each length of each part matches expected length from schema\n",
    "        if len(part) not in length_options:\n",
    "            raise ValueError(f\"Part {part} does not have expected length {len(part)}\")\n",
    "        # Check that each part matches expected pattern from schema\n",
    "        if not any(re.fullmatch(pattern, part) for pattern in pattern_options):\n",
    "            raise ValueError(f\"Part {part} does not match expected patterns {pattern_options}\")\n",
    "\n",
    "        # Special handling of a part (pol orbit) that has 3 types of metadata\n",
    "        if name == \"pol_orbit\":\n",
    "            parsed_data.update(\n",
    "                {\n",
    "                    \"polarization_type\": part[:1],  # Single (S) or Dual (D) pol\n",
    "                    \"primary_polarization\": part[1:2],  # Primary polarization (H or V)\n",
    "                    \"orbit_type\": part[-1],  # Precise (p), Restituted (r) or Original predicted (o)\n",
    "                }\n",
    "            )\n",
    "        # Format string acquisition date as a datetime time stamp\n",
    "        elif name == \"acq_date\":\n",
    "            parsed_data[name] = pd.to_datetime(part, format=\"%Y%m%dT%H%M%S\")\n",
    "        # Expand multiple variables stored in output_info string part\n",
    "        elif name == \"output_info\":\n",
    "            output_info_keys = [\n",
    "                \"output_type\",\n",
    "                \"output_unit\",\n",
    "                \"unmasked_or_watermasked\",\n",
    "                \"notfiltered_or_filtered\",\n",
    "                \"area_or_clipped\",\n",
    "                \"deadreckoning_or_demmatch\",\n",
    "            ]\n",
    "\n",
    "            output_info_values = [part[0], part[1], part[2], part[3], part[4], part[-1]]\n",
    "\n",
    "            parsed_data.update(dict(zip(output_info_keys, output_info_values)))\n",
    "\n",
    "        else:\n",
    "            parsed_data[name] = part\n",
    "\n",
    "    # Because we have already addressed product type in the variable names\n",
    "    prod_type = parsed_data.pop(\"prod_type\")\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we've already addressed product type ('VV' (dual-pol), 'VH'(cross-pol), or 'LS' (layover-shadow)) in the variable names, we do not need to add it from the file name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "We want to call `parse_fname_metadata()` on every GeoTIFF file in the time series. This means that we need the lists of file paths for each variable stored as GeoTIFF files that were created in the last notebook. \n",
    "\n",
    "Before calling `parse_fname_metadata()`, we recreate the lists by calling the same functions used in the last notebook from `s1_tools`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_ls = [\"vv\", \"vh\", \"ls_map\", \"readme\"]\n",
    "# Make dict\n",
    "filepaths_dict = s1_tools.create_filenames_dict(path_to_rtcs, variables_ls)\n",
    "# Separate into lists\n",
    "filepaths_vv, filepaths_vh, filepaths_ls, filepaths_rm = filepaths_dict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Extract and format acquisition dates\n",
    "\n",
    "From each list of dictionaries created above, look at only the `acq_date` key, value pair in order to create lists of acquisition dates for each GeoTIFF file in the stack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_dates_vh = [\n",
    "    parse_fname_metadata(filepaths_vh[file])[\"acq_date\"].strftime(\"%m/%d/%YT%H%M%S\")\n",
    "    for file in range(len(filepaths_vh))\n",
    "]\n",
    "acq_dates_vv = [\n",
    "    parse_fname_metadata(filepaths_vv[file])[\"acq_date\"].strftime(\"%m/%d/%YT%H%M%S\")\n",
    "    for file in range(len(filepaths_vv))\n",
    "]\n",
    "acq_dates_ls = [\n",
    "    parse_fname_metadata(filepaths_ls[file])[\"acq_date\"].strftime(\"%m/%d/%YT%H%M%S\")\n",
    "    for file in range(len(filepaths_ls))\n",
    "]\n",
    "\n",
    "# Make sure they are  equal\n",
    "assert acq_dates_vh == acq_dates_vv == acq_dates_ls, \"Acquisition dates lists for VH, VV and L-S Map do not match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pause and return to the data we read at the very beginning and look at how it relates to the acquisition date metadata we just parsed.\n",
    "\n",
    "In each data cube object (`ds_vv`, `ds_vh` and `ds_ls`), there is a `'band'` dimension that has a length of 103. We know that this corresponds to time because of how we created the VRT files, but we don't know anything else about the timing of the observations stored in the data cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also just created three lists: `acq_dates_vv`, `acq_dates_vh`, and `acq_dates_ls`. Each element of these lists contains a datetime-like object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acq_dates_vv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign these lists of dates to the data cubes using the `xr.assign_coords()` method together with `pd.to_datetime()`. [`pd.to_datetime()`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) takes a string (the list elements) and turns it into a ['time-aware' object](https://pandas.pydata.org/docs/user_guide/timeseries.html). [`xr.assign_coords()](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.assign_coords.html) assigns an array to a given coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv = ds_vv.assign_coords({\"band\": pd.to_datetime(acq_dates_vv, format=\"%m/%d/%YT%H%M%S\")}).rename(\n",
    "    {\"band\": \"acq_date\"}\n",
    ")\n",
    "ds_vh = ds_vh.assign_coords({\"band\": pd.to_datetime(acq_dates_vh, format=\"%m/%d/%YT%H%M%S\")}).rename(\n",
    "    {\"band\": \"acq_date\"}\n",
    ")\n",
    "ds_ls = ds_ls.assign_coords({\"band\": pd.to_datetime(acq_dates_ls, format=\"%m/%d/%YT%H%M%S\")}).rename(\n",
    "    {\"band\": \"acq_date\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we're assigning the time-aware array produced by passing `acq_dates_vv` to `pd.to_datetime()` to the `'band'` coordinate of the raster data cube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Combine data cubes\n",
    "\n",
    "Up until this point, we've been working with three raster data cube objects, one for each variable (`vv`, `vh` and `ls`). We can combine them into one data cube with multiple variables using [`xr.combine_by_coords()`](https://docs.xarray.dev/en/stable/generated/xarray.combine_by_coords.html#xarray.combine_by_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.combine_by_coords([ds_vv, ds_vh, ds_ls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the first notebook, the scenes are not read in temporal order when the VRT objects were created, which is why the current data cube is not in temporal order. This is okay, because while the scenes were not in temporal order, they were read in the *same* order across all variables, which is why we could create and assign the time dimensions (`acq_date`) as we did and merge the individual objects into one data cube. \n",
    "\n",
    "Now, let's use [`xr.sortby()`](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.sortby.html) to arrange the data cube's time dimension in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.sortby(\"acq_date\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Time-varying metadata\n",
    "\n",
    "So far we've only used the acquisition dates extracted from the file names, but the function we wrote `parse_fname_metadata()` holds additional information about how the scenes were imaged and processed. \n",
    "\n",
    "Similarly to how we attached the acquisition date information to the raster data cubes, we'd also like to organize the imaging and processing metadata so that it is stored in relation to the raster imagery in a way that helps us interpret the raster imagery.\n",
    "\n",
    "Remember that we have a dictionary of metadata attributes for each step of the time series and each variable (product type). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_fname_metadata(filepaths_vv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than store this information as `attrs` of the data cube as a whole (`ds.attrs`) or individual data variables(`ds['vv'].attrs`), **this information should be stored as coordinate variables of the data cube**. This way, there is a coordinate name (the key of each dict item) and an array that holds the value at each time step. \n",
    "\n",
    "The next several steps demonstrate how to execute this.\n",
    "\n",
    "### 1) Extract metadata from file name for each GeoTIFF file\n",
    "\n",
    "Because product type is handled in the variable names, it isn't necessary to extract from the filename. We'll verify that the parsed metadata dictionaries are identical across variables and then move forward with only one list of metadata dictionaries. To do this, apply `parse_fname_metadata()` to each element of each variable list, making a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_attrs_list_vv = [parse_fname_metadata(filepaths_vv[file]) for file in range(len(filepaths_vv))]\n",
    "\n",
    "meta_attrs_list_vh = [parse_fname_metadata(filepaths_vh[file]) for file in range(len(filepaths_vh))]\n",
    "\n",
    "meta_attrs_list_ls = [parse_fname_metadata(filepaths_ls[file]) for file in range(len(filepaths_ls))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that they are all identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    meta_attrs_list_ls == meta_attrs_list_vh == meta_attrs_list_vv\n",
    "), \"Lists of metadata dicts should be identical for all variables\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use only one list moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Re-organize lists of metadata dictionaries\n",
    "\n",
    "To assign coordinate variables to an Xarray dataset, the metadata needs to be re-organized a bit more. Currently, we have lists of 103 dictionaries with identical keys; each key will become a coordinate variable. We need to transpose the initial list of dictionaries; instead of a list of 103 dictionaries with 14 keys, we want a dictionary with 14 keys where each value is a 103-element long list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_metadata_dict_list(input_ls: list) -> dict:\n",
    "    df_ls = []\n",
    "    # Iterate through list\n",
    "    for element in range(len(input_ls)):\n",
    "        # Create a dataframe of each dict in the list\n",
    "        item_df = pd.DataFrame(input_ls[element], index=[0])\n",
    "        df_ls.append(item_df)\n",
    "    # Combine dfs row-wise into one df\n",
    "    attr_df = pd.concat(df_ls)\n",
    "    # Separate out each column in df to its own dict\n",
    "    attrs_dict = {col: attr_df[col].tolist() for col in attr_df.columns}\n",
    "    # Acq_dates are handled separately since we will use it as an index\n",
    "    acq_date_ls = attrs_dict.pop(\"acq_date\")\n",
    "    return (attrs_dict, acq_date_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_dict, acq_dt_list = transpose_metadata_dict_list(meta_attrs_list_vv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Create `xr.Dataset` with metadata as coordinate variables\n",
    "We've re-organized the metadata into a format that Xarray expects for a `xr.DataArray` object. Because we will create a coordinate `xr.DataArray` for each metadata key, we write a function to repeat the operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_da(value_name: str, values_ls: list, dim_name: str, dim_values: list, desc: str = None) -> xr.DataArray:\n",
    "    \"\"\"Given a list of metadata values, create a 1-d xr.DataArray with values\n",
    "    as data that exists along a specified dimension (here, hardcoded to be\n",
    "    acq_date). Optionally, add description of metadata as attr.\n",
    "    Returns a xr.DataArray\"\"\"\n",
    "    da = xr.DataArray(\n",
    "        data=values_ls,\n",
    "        dims=[dim_name],\n",
    "        coords={dim_name: dim_values},\n",
    "        name=value_name,\n",
    "    )\n",
    "    if desc != None:\n",
    "        da.attrs = {\"description\": desc}\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply it to all of the metadata keys and then combine the `xr.DataArray` objects into a `xr.Dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_ds(dict_of_attrs: dict, list_of_acq_dates: list) -> xr.Dataset:\n",
    "    da_ls = []\n",
    "    for key in dict_of_attrs.keys():\n",
    "        da = create_da(key, dict_of_attrs[key], \"acq_date\", list_of_acq_dates)\n",
    "        da_ls.append(da)\n",
    "\n",
    "    coord_ds = xr.combine_by_coords(da_ls)\n",
    "    coord_ds = coord_ds.sortby(\"acq_date\")\n",
    "    return coord_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_ds = create_metadata_ds(attr_dict, acq_dt_list)\n",
    "coord_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we successfully extracted the data from the file names, organized it as dictionaries and created a `xr.Dataset` that holds all of the information organized along the appropriate dimension. The last step is to assign the data variables in `coord_ds` to be coordinate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_ds = coord_ds.assign_coords({k: v for k, v in dict(coord_ds.data_vars).items()})\n",
    "coord_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, combine them with the original data cube of backscatter imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Combine `xr.Dataset` of backscatter data with `xr.Dataset` of coordinate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata = xr.merge([ds, coord_ds])\n",
    "ds_w_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, from a stack of files that make up a raster time series, we figured out how to take information stored in a every file name and assign it as non-dimensional coordinate variables of the data cube representing the raster time series. \n",
    "\n",
    "While it's good that we did this because there is nothing *ensuring* that the metadata in the file names is identical across time steps, many of these attributes are identical across the time series. This means that we can handle them more simply as attributes rather than coordinate variables, though we will still keep some metadata as coordinates. \n",
    "\n",
    "First, verify which metadata attributes are identical throughout the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of coords that are along time dimension\n",
    "coords_along_time_dim = [coord for coord in ds_w_metadata._coord_names if \"acq_date\" in ds_w_metadata[coord].dims]\n",
    "dynamic_attrs = []\n",
    "static_attr_dict = {}\n",
    "for i in coords_along_time_dim:\n",
    "    if i != \"acq_date\":\n",
    "        # find coordinate array variables that have more than\n",
    "        # one unique element along time dimension\n",
    "        if len(set(ds_w_metadata.coords[i].data.tolist())) > 1:\n",
    "            dynamic_attrs.append(i)\n",
    "        else:\n",
    "            static_attr_dict[i] = ds_w_metadata.coords[i].data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the static coordinates from the data cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata = ds_w_metadata.drop_vars(list(static_attr_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add them as attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata.attrs = static_attr_dict\n",
    "ds_w_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Add metadata from README file\n",
    "\n",
    "Until now, all of the attribute information that we've added to the data cube came from file names of the individual scenes that make up the data cube. One piece of information not included in the file name is the original granule ID of the scene published by ESA. This is information about the source image, produced by ESA, and used by ASF when processing a SLC image into a RTC image. This is located in the README file within each scene directory. \n",
    "\n",
    "\n",
    "Having the granule ID can be helpful because it tells you if the satellite was in an ascending or a descending cycle when the image was captured. For surface conditions with diurnal fluctuations such as snowmelt, the timing of the image acquisition may be important.\n",
    "\n",
    ":::{note} \n",
    "This notebook works through the steps to format and organize metadata while explaining the details behind these operations. It is meant to be an educational demonstration. There are packages available such as [Sentinelsat](https://sentinelsat.readthedocs.io/en/stable/) that simplify metadata retrieval of Sentinel-1 imagery.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Extract granule ID\n",
    "\n",
    "We first need to extract the text containing a scene's granule ID from that scene's README file. This is similar to parsing the file and directory names but this time we're extracting text from a markdown file. To do this, we use the [python-markdown](https://python-markdown.github.io/) package. From looking at the file, we know where the source granule information is located; we need to isolate it from the rest of the text in the file. The source granule information always follows the same pattern, so we can use string methods to extract it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_granule_id(filepath):\n",
    "    \"\"\"takes a filepath to the readme associated with an S1 scene and returns the source granule id used to generate the RTC imagery\"\"\"\n",
    "\n",
    "    # Use markdown package to read text from README\n",
    "    md = markdown.Markdown(extensions=[\"meta\"])\n",
    "    # Extract text from file\n",
    "    data = pathlib.Path(filepath).read_text()\n",
    "    # this text precedes granule ID in readme\n",
    "    pre_gran_str = \"The source granule used to generate the products contained in this folder is:\\n\"\n",
    "    split = data.split(pre_gran_str)\n",
    "    # isolate the granule id\n",
    "    gran_id = split[1][:67]\n",
    "\n",
    "    return gran_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, take a look at a single source granule ID string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "granule_ls = [extract_granule_id(filepaths_rm[element]) for element in range(len(filepaths_rm))]\n",
    "granule_ls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is information from the ESA's [SentiWiki](https://sentiwiki.copernicus.eu/web/) on [Sentinel-1 Product Naming Conventions](https://sentiwiki.copernicus.eu/web/s1-products#S1Products-SARNamingConventionS1-Products-SAR-Naming-Convention). In addition to the sensor and the beam mode, it tells us that the product type is a 'Single Look Complex' (SLC) image and is Level-1 processed data. The two date-time like sections indicate the start (14 February 2022, 12:13:53) and stop (14 February 2022, 12:14:20) date and time of the observation which are followed by the absolute orbit number (041909), a mission data take ID (04FD6F) and a unique product ID (42AF). \n",
    "\n",
    "```{image} ../imgs/esa_s1_naming.png\n",
    "align: center\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Build granule ID coordinate array\n",
    "\n",
    "This is another situation of time-varying metadata. Once we have the individual granule IDs, we must format them so that they can be attached to an Xarray object. We use a similar schema approach to parsing the file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coord_data(readme_fpaths_ls):\n",
    "    \"\"\"takes a list of the filepaths to every read me, extracts the granule ID.\n",
    "    From granule ID, extracts acquisition date and data take ID.\n",
    "    Returns a tuple of lists of acquisition dates and data take IDs.\"\"\"\n",
    "\n",
    "    # Make a list of all granules in time series\n",
    "    granule_ls = [extract_granule_id(readme_fpaths_ls[element]) for element in range(len(readme_fpaths_ls))]\n",
    "    # Define a schema for acquisition date\n",
    "    schema = {\n",
    "        \"mission_identifier\": (3, r\"S1[A-B]\"),  # schema for sensor\n",
    "        \"mode_beam_identifier\": (2, r\"[A-Z]{2}\"),  # schema for beam mode\n",
    "        \"esa_product_type\": (3, r\"[A-Z]{3}\"),  # schema for ESA product type\n",
    "        \"proc_lvl_class_pol\": (4, r\"[A-Z0-9]{{4}}\"),\n",
    "        \"acq_start\": (15, r\"[0-9]{8}T[0-9]{6}\"),  # schema for acquisition dat\n",
    "        \"acq_stop\": (15, r\"[0-9]{8}T[0-9]{6}\"),  # schema for acquisition dat\n",
    "        \"orbit_num\": (6, r\"[0-9]{6}\"),  # schema for orbit number\n",
    "        \"data_take_id\": (6, \"A-Z0-9{6}\"),  # schema for data take id\n",
    "    }\n",
    "\n",
    "    # Extract relevant metadata from granule ID\n",
    "    all_granules_parsed_data = []\n",
    "    for granule in granule_ls:\n",
    "        # need to account for double under score\n",
    "        parts = [s for s in granule.split(\"_\") if len(s) > 0]\n",
    "        # parts = granule.split(\"_\")\n",
    "        single_granule_parsed_data = {}\n",
    "        for part, (name, (length, pattern)) in zip(parts, schema.items()):\n",
    "            if name == \"acq_start\":\n",
    "                single_granule_parsed_data[\"acq_start\"] = pd.to_datetime(part, format=\"%Y%m%dT%H%M%S\")\n",
    "            elif name == \"orbit_num\":\n",
    "                single_granule_parsed_data[name] = part\n",
    "            elif name == \"data_take_id\":\n",
    "                single_granule_parsed_data[name] = part\n",
    "        all_granules_parsed_data.append(single_granule_parsed_data)\n",
    "\n",
    "    acq_dates = [granule[\"acq_start\"] for granule in all_granules_parsed_data]\n",
    "    abs_orbit_no = [granule[\"orbit_num\"] for granule in all_granules_parsed_data]\n",
    "    data_take_ids = [granule[\"data_take_id\"] for granule in all_granules_parsed_data]\n",
    "\n",
    "    return (acq_dates, abs_orbit_no, data_take_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_dates, abs_orbit_ls, data_take_ids_ls = make_coord_data(filepaths_rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the `create_da()` function that we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_take_id_da = create_da(\n",
    "    value_name=\"data_take_id\",\n",
    "    values_ls=data_take_ids_ls,\n",
    "    dim_name=\"acq_date\",\n",
    "    dim_values=acquisition_dates,\n",
    "    desc=\"ESA Mission data take ID\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure `data_take_id_da` is sorted in chronological order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_take_id_da = data_take_id_da.sortby(\"acq_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `xr.DataArray` of ESA data take IDs to raster data cube of Sentinel-1 scenes as a coordinate variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata.coords[\"data_take_ID\"] = data_take_id_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above steps to add absolute orbit number as a coordinate variable to the data cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DatArray\n",
    "abs_orbit_da = create_da(\n",
    "    value_name=\"abs_orbit_num\",\n",
    "    values_ls=abs_orbit_ls,\n",
    "    dim_name=\"acq_date\",\n",
    "    dim_values=acquisition_dates,\n",
    "    desc=\"Absolute orbit number\",\n",
    ")\n",
    "# Sort by time\n",
    "abs_orbit_da = abs_orbit_da.sortby(\"acq_date\")\n",
    "# Add as coord variable to data cube\n",
    "ds_w_metadata.coords[\"abs_orbit_num\"] = abs_orbit_da\n",
    "ds_w_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. 3-d metadata\n",
    "\n",
    "The metadata we've looked at so far has been information related to how and when the image was processed. An element of the dataset we haven't focused on yet is the layover shadow map provided by ASF. This is an important tool for interpreting backscatter imagery that we'll discuss in more detail in the next notebook. What's important to know at this step is that the layover-shadow map is provided for every scene in the time series and contains a number assignment for each pixel in the scene that indicates the presence or absence of layover, shadow, and slope angle conditions that impact backscatter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Set data variable containing metadata as coordinate variable\n",
    "This means that the layover-shadow map is a raster image in itself that helps us interpret the raster images of backscatter values. Because it is metadata that gives context to the physical observable (backscatter) and it varies over spatial and temporal dimensions, it is most appropriately represented as a coordinate variable of the data cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata = ds_w_metadata.set_coords(\"ls\")\n",
    "ds_w_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walked through an in-depth example of what working with real-world datasets can look like and the steps that are required to prepare data for analysis. We went from individual datasets for each variables that lacked important information about physical observables (the image below shows an unformatted data cube of the *full* time series):\n",
    "\n",
    "```{image} ../imgs/init_data_cube.png\n",
    ":align center\n",
    "```\n",
    "To a single data cube containing all three variables with a temporal dimension of `np.datetime64` objects and additional metadata formatted as both coordinate variables and attributes depending on if it is static in time or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will now be much easier to make use of the dataset's metadata in our analysis whether that is querying by time or space, performing computations on multiple variables at a time or filtering by unique ID's of the processed and source datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
