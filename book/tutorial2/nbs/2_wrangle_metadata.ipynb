{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{title_s1_2}}\n",
    "\n",
    "{{intro}}\n",
    "\n",
    "In the last notebook, we demonstrated reading a larger-than-memory dataset using GDAL virtual datasets. However, reading the data in this way caused important metadata to be lost. This notebook details steps of parsing important metadata stored in filenames and separate files and formatting it so that it can be combined with the `Xr.Dataset` holding the physical observables, Sentinel-1 backscatter imagery for two different polarizations and a layover-shadow mask. \n",
    "\n",
    "::::{tab-set}\n",
    ":::{tab-item} Outline  \n",
    "(content.Section_A)=\n",
    "**[A. Read and inspect initial metadata](#a-read-data-and-inspect-initial-metadata)**\n",
    "\n",
    "- 1) Rename `band_data` variables to appropriate names  \n",
    "- 2) What metadata currently exists?  \n",
    "\n",
    "(content.Section_B)=\n",
    "**[B. Add metadata from file name](#b-add-metadata-from-file-name)**  \n",
    "\n",
    "- 1) Parse file name  \n",
    "- 2) Extract and format acquisition dates   \n",
    "- 3) Combine data cubes \n",
    "\n",
    "(content.Section_C)=\n",
    "**[C. Time-varying metadata](#c-time-varying-metadata)** \n",
    "\n",
    "- 1) Apply `parse_fname_metadata()` to each element of each variable list, making a list of dictionaries   \n",
    "- 2) Create tuple of metadata for each type of information  \n",
    "- 3) Assign metadata tuple to a coordinate variable that exists along the time dimension of an Xarray dataset \n",
    "\n",
    "(content.Section-D)=\n",
    "**[D. Add metadata from README file](#d-add-metadata-from-readme-file)**  \n",
    "\n",
    "- 1) Extract granule ID  \n",
    "- 2) Build granule ID coordinate array  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cf_xarray\n",
    "import geopandas as gpd\n",
    "import markdown\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import re\n",
    "import xarray as xr\n",
    "\n",
    "import s1_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "tutorial2_dir = pathlib.Path(cwd).parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Read data and inspect initial metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the GDAL virtual dataset combines the stack of images by assigning each image to a `'band'` dimension. However, while we now have the data read into memory, we *don't* have important metadata that we need in order to interpret the data.\n",
    "\n",
    "A good first step here is to check out the README file that is located within each scene directory. This has important metadata that we need as well as instructuions for how to interpret the file names. Check that out now if you haven't yet. \n",
    "\n",
    "Next, we have to figure out how to get the information from the README files and the file names and organize it so that it is most useful to us as we try to interpret the Sentinel-1 backscatter imagery. \n",
    "\n",
    "First, read the data using Xarray and the VRT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv = xr.open_dataset('../data/vrt_files/s1_stackVV.vrt', chunks='auto').squeeze()\n",
    "ds_vh = xr.open_dataset('../data/vrt_files/s1_stackVH.vrt', chunks='auto')\n",
    "ds_ls = xr.open_dataset('../data/vrt_files/s1_stackLS.vrt', chunks='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Rename `band_data` variables to appropriate names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv = ds_vv.rename({'band_data':'vv'})\n",
    "ds_vh = ds_vh.rename({'band_data':'vh'})\n",
    "ds_ls = ds_ls.rename({'band_data':'ls'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) What metadata currently exists? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `cf_xarray` to parse the metadata contained in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv.cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we're lacking a lot of information, but we do have spatial reference information that allows us to understand how the data cube's pixels correspond to locations on Earth's surface (we also know this from the README file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `x` and `y` dimensions have associated coordinate variables and there is a `'spatial_ref'` variable that gives coordinate reference system information about those coordinates. In the attributes of `'spatial_ref'`, we see that our object is projected in local UTM (Universal Transverse Mercator) zone 45N. This tells us that the units of the coordinate arrays are meters. Read more about UTM zones [here](https://www.usgs.gov/faqs/what-does-term-utm-mean-utm-better-or-more-accurate-latitudelongitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv['spatial_ref'].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we know where our data is located in space. But, with the data in its current form, we don't know much else about how or when it was collected. The rest of this notebook will demonstrate finding that information from both the file names of the individual files and the other files located in the scene directories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Add metadata from file name \n",
    "\n",
    "Perhaps most importantly, we need to know when these observations were collected in time.\n",
    "\n",
    "As we know from the README, information about acquisitions dates and more is stored in the directory name for each scene as well as the individual file names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vh.band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Parse file name\n",
    "The function below extracts metadata from the filename of each GeoTIFF file in the time series. It uses [regex expressions](https://docs.python.org/3/howto/regex.html) to parse the different types of information stored within the file name based on the instructions from the README. Once parsed, the information is stored as a dictionary where each key is the type of information (ie. Acquisition start date and time, polarization, imaging mode, etc.) and each value is the corresponding information from the file name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fname_metadata(input_fname: str) -> dict:\n",
    "    \"\"\"Function to extract information from filename and separate into expected variables based on a defined schema.\"\"\"\n",
    "    # Define schema\n",
    "    schema = {\n",
    "        \"sensor\": (3, r\"S1[A-B]\"),  # schema for sensor\n",
    "        \"beam_mode\": (2, r\"[A-Z]{2}\"),  # schema for beam mode\n",
    "        \"acq_date\": (15, r\"[0-9]{8}T[0-9]{6}\"),  # schema for acquisition date\n",
    "        \"pol_orbit\": (3, r\"[A-Z]{3}\"),  # schema for polarization + orbit type\n",
    "        \"terrain_correction_pixel_spacing\": (\n",
    "            5,\n",
    "            r\"RTC[0-9]{2}\",\n",
    "        ),  # schema for terrain correction pixel spacing\n",
    "        \"processing_software\": (\n",
    "            1,\n",
    "            r\"[A-Z]{1}\",\n",
    "        ),  # schema for processing software (G = Gamma)\n",
    "        \"output_info\": (6, r\"[a-z]{6}\"),  # schema for output info\n",
    "        \"product_id\": (4, r\"[A-Z0-9]{4}\"),  # schema for product id\n",
    "        \"prod_type\": ((2, 6), (r\"[A-Z]{2}\", r\"ls_map\")),  # schema for polarization type\n",
    "    }\n",
    "\n",
    "    # Remove prefixs\n",
    "    input_fname = input_fname.split(\"/\")[-1]\n",
    "    # Remove file extension if present\n",
    "    input_fname = input_fname.removesuffix(\".tif\")\n",
    "    # Split filename string into parts\n",
    "    parts = input_fname.split(\"_\")\n",
    "\n",
    "    # l-s map objects have an extra '_' in the filename. Remove/combine parts so that it matches schema\n",
    "    if parts[-1] == \"map\":\n",
    "        parts = parts[:-1]\n",
    "        parts[-1] = parts[-1] + \"_map\"\n",
    "\n",
    "    # Check that number of parts matches expected schema\n",
    "    if len(parts) != len(schema):\n",
    "        raise ValueError(\n",
    "            f\"Input filename does not match schema of expected format: {parts}\"\n",
    "        )\n",
    "\n",
    "    # Create dict to store parsed data\n",
    "    parsed_data = {}\n",
    "\n",
    "    # Iterate through parts and schema\n",
    "    for part, (name, (length_options, pattern_options)) in zip(parts, schema.items()):\n",
    "        # In the schema we defined, items have an int for length or a tuple (when there is more than one possible lenght)\n",
    "        # Make the int lengths into tuples\n",
    "        if isinstance(length_options, int):\n",
    "            length_options = (length_options,)\n",
    "        # Same as above for patterns\n",
    "        if isinstance(pattern_options, str):\n",
    "            pattern_options = (pattern_options,)\n",
    "\n",
    "        # Check that each length of each part matches expected length from schema\n",
    "        if len(part) not in length_options:\n",
    "            raise ValueError(f\"Part {part} does not have expected length {len(part)}\")\n",
    "        # Check that each part matches expected pattern from schema\n",
    "        if not any(re.fullmatch(pattern, part) for pattern in pattern_options):\n",
    "            raise ValueError(\n",
    "                f\"Part {part} does not match expected patterns {pattern_options}\"\n",
    "            )\n",
    "\n",
    "        # Special handling of a part (pol orbit) that has 3 types of metadata\n",
    "        if name == \"pol_orbit\":\n",
    "            parsed_data.update(\n",
    "                {\n",
    "                    \"polarization_type\": part[:1],  # Single (S) or Dual (D) pol\n",
    "                    \"primary_polarization\": part[1:2],  # Primary polarization (H or V)\n",
    "                    \"orbit_type\": part[\n",
    "                        -1\n",
    "                    ],  # Precise (p), Restituted (r) or Original predicted (o)\n",
    "                }\n",
    "            )\n",
    "        # Format string acquisition date as a datetime time stamp\n",
    "        elif name == \"acq_date\":\n",
    "            parsed_data[name] = pd.to_datetime(part, format=\"%Y%m%dT%H%M%S\")\n",
    "        # Expand multiple variables stored in output_info string part\n",
    "        elif name == \"output_info\":\n",
    "            output_info_keys = [\n",
    "                \"output_type\",\n",
    "                \"output_unit\",\n",
    "                \"unmasked_or_watermasked\",\n",
    "                \"notfiltered_or_filtered\",\n",
    "                \"area_or_clipped\",\n",
    "                \"deadreckoning_or_demmatch\",\n",
    "            ]\n",
    "\n",
    "            output_info_values = [part[0], part[1], part[2], part[3], part[4], part[-1]]\n",
    "\n",
    "            parsed_data.update(dict(zip(output_info_keys, output_info_values)))\n",
    "\n",
    "        else:\n",
    "            parsed_data[name] = part\n",
    "\n",
    "    #remove prod type from parsed data dict\n",
    "    prod_type = parsed_data.pop('prod_type')\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "We want to call `parse_fname_metadata()` on every GeoTIFF file in the time series. This means that we need the lists of file paths for each variable stored as GeoTIFF files that were created in the last notebook. \n",
    "\n",
    "Before calling `parse_fname_metadata()`, we recreate the lists by calling the same functions used in the last notebook from `s1_tools`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to directory holding downloaded data\n",
    "s1_asf_data = pathlib.Path(cwd.parents[3], 'sentinel1_rtc/data/asf_rtcs')\n",
    "# Make file path lists for vv, vh, ls\n",
    "filepaths_vv, filepaths_vh, filepaths_ls, filepaths_rm = s1_tools.make_filename_lists(s1_asf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Extract and format acquisition dates\n",
    "\n",
    "From each list of dictionaries created above, look at only the `acq_date` key, value pair in order to create lists of acquisition dates for each GeoTIFF file in the stack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_dates_vh = [\n",
    "    parse_fname_metadata(filepaths_vh[file])[\"acq_date\"].strftime(\"%m/%d/%YT%H%M%S\")\n",
    "    for file in range(len(filepaths_vh))\n",
    "]\n",
    "acq_dates_vv = [\n",
    "    parse_fname_metadata(filepaths_vv[file])[\"acq_date\"].strftime(\"%m/%d/%YT%H%M%S\")\n",
    "    for file in range(len(filepaths_vv))\n",
    "]\n",
    "acq_dates_ls = [\n",
    "    parse_fname_metadata(filepaths_ls[file])[\"acq_date\"].strftime(\"%m/%d/%YT%H%M%S\")\n",
    "    for file in range(len(filepaths_ls))\n",
    "]\n",
    "\n",
    "#Make sure they are  equal\n",
    "assert acq_dates_vh == acq_dates_vv == acq_dates_ls, (\n",
    "    \"Acquisition dates lists for VH, VV and L-S Map do not match\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pause and return to the data we read at the very beginning and look at how it relates to the acquisition date metadata we just parsed.\n",
    "\n",
    "In each data cube object (`ds_vv`, `ds_vh` and `ds_ls`) there is a `'band'` dimension that has a length of 103. We know that this corresponds to time because of how we created the VRT files, but we don't know anything else about the timing of the observations stored in the data cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also just created three lists: `acq_dates_vv`, `acq_dates_vh`, and `acq_dates_ls`. Each element of these lists contains a datetime-like object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_dates_vv[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign these lists of dates to the data cubes using the `xr.assign_coords()` method together with `pd.to_datetime()`. [`pd.to_datetime()`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) takes a string (the list elements) and turns it into a ['time-aware' object](https://pandas.pydata.org/docs/user_guide/timeseries.html). [`xr.assign_coords()](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.assign_coords.html) assigns an array to a given coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vv = ds_vv.assign_coords(\n",
    "    {\"band\": pd.to_datetime(acq_dates_vv, format=\"%m/%d/%YT%H%M%S\")}\n",
    ").rename({'band':'acq_date'})\n",
    "ds_vh = ds_vh.assign_coords(\n",
    "    {\"band\": pd.to_datetime(acq_dates_vh, format=\"%m/%d/%YT%H%M%S\")}\n",
    ").rename({'band':'acq_date'})\n",
    "ds_ls = ds_ls.assign_coords(\n",
    "    {\"band\": pd.to_datetime(acq_dates_ls, format=\"%m/%d/%YT%H%M%S\")}\n",
    ").rename({'band':'acq_date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we're assigning the time-aware array produced by passing `acq_dates_vv` to `pd.to_datetime()` to the `'band'` coordinate of the raster data cube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Combine data cubes\n",
    "\n",
    "Up until this point, we've been working with three raster data cube objects, one for each variable (`vv`, `vh` and `ls`). We can combine them into one data cube with multiple variables using [`xr.combine_by_coords()`](https://docs.xarray.dev/en/stable/generated/xarray.combine_by_coords.html#xarray.combine_by_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.combine_by_coords([ds_vv, ds_vh, ds_ls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the first notebook, the scenes are not read in temporal order when the VRT objects were created, which is why the current data cube is not in temporal order. This is okay, because while the scenes were not in temporal order, they were read in the *same* order across all variables, which is why we could create and assign the time dimensions (`acq_date`) as we did and merge the individual objects into one data cube. \n",
    "\n",
    "Now, let's use [`xr.sortby()`](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.sortby.html) to arrange the data cube's time dimension in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.sortby('acq_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data cube is much more usable than when we first read it into memory. We've combined the multiple objects into one, replaced the `'band'` dimension that contained an array of arbitrary integers with a `'acq_date'` dimension of [`'datetime64[ns]'`](https://numpy.org/doc/2.2/reference/arrays.datetime.html) objects and sorted the data cube in chronological order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} Taking a look at chunking\n",
    "\n",
    "```\n",
    "{image} ../imgs/s1_chunks.png\n",
    ":alt chunks_image\n",
    ":align_center\n",
    "```\n",
    "\n",
    "Each variable in `ds` has a total shape of (103,13379,1742) and is 89.59 GB. It is chunked so that each chunk is (11,1536,1536) and 99 MB, with 1080 total chunks per variable. \n",
    "\n",
    "Depending on your use-case, you may want to adjust the chunking of the object. For example, if you are interested in analyzing variability along the temporal dimension, it might make sense to re-chunk the dataset so that operations along the that dimension are more easily parallelized. For more detail, see the chunking discussion in Tutorial 1.  \n",
    "(TODO, move this to general intro?) and the [Parallel Computing with Dask](https://docs.xarray.dev/en/stable/user-guide/dask.html) section of the Xarray documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Time-varying metadata\n",
    "\n",
    "So far we've only used the acquisition dates extracted from the file names, but the function we wrote `parse_fname_metadata()` holds additional information about how the scenes were imaged and processed. \n",
    "\n",
    "Similarly to how we attached the acquisition date information to the raster data cubes, we'd also like to organize the imaging and processing metadata so that it is stored in relation to the raster imagery in a way that helps us interpret the raster imagery.\n",
    "\n",
    "Remember that we have a dictionary of metadata attributes for each step of the time series and each variable (product type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_fname_metadata(filepaths_vv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_fname_metadata(filepaths_vh[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than store this information as `attrs` of the data cube as a whole (`ds.attrs`) or individual data variables(`ds['vv'].attrs`), **this information should be stored as coordinate variables of the data cube**. This way, there is a coordinate name (the key of each dict item) and an array that holds the value at each time step. \n",
    "\n",
    "The next several steps demonstrate how to execute this.\n",
    "\n",
    "### 1) Apply `parse_fname_metadata()` to each element of each variable list, making a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_attrs_list_vv = [\n",
    "    parse_fname_metadata(filepaths_vv[file]) for file in range(len(filepaths_vv))\n",
    "]\n",
    "\n",
    "meta_attrs_list_vh = [\n",
    "    parse_fname_metadata(filepaths_vh[file]) for file in range(len(filepaths_vh))\n",
    "]\n",
    "\n",
    "meta_attrs_list_ls = [\n",
    "    parse_fname_metadata(filepaths_ls[file]) for file in range(len(filepaths_ls))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(meta_attrs_list_vv[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Create tuple of metadata for each type of information\n",
    "\n",
    "To assign coordinate variables to an Xarray dataset, the metadata needs to be re-organized a bit more. Currently, we have a list of 103 dictionaries with identical keys; each key will become a coordinate variable. To assign a coordinate variable to a `xr.Dataset`, Xarray expects a tuple with the following form: `(coordinate name, coordinate array)` where the array is all of the values for a single key from all dictionaries in the list.  \n",
    "\n",
    "In short, we need to go from 103 dictionaries with 14 keys each to 14 tuples, each holding the key name and an array with length 103."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_meta_tuple(input_ls):\n",
    "    \"\"\"takes a list of dictionaries where each dict is metadata for a given time step,\n",
    "    returns a tuple. input list is re-organized to a tuple where each [0] values is a metadata category (ie. sensor) and\n",
    "    [1] is a time series of the categories values over time (ie. S1A, S1A, S1A ...)\"\"\"\n",
    "\n",
    "    #Start wtih first dictionary in list\n",
    "    meta_dict = input_ls[0]\n",
    "    ticker = 0\n",
    "    #Initialize empty lists to hold attr dictionaries and keys\n",
    "    attrs_dicts, keys_ls = [], []\n",
    "\n",
    "    #Iterate through keys in first dictionary\n",
    "    for key in meta_dict:\n",
    "        if key == \"acq_date\": #Skip, already added as a dimensional coordinate variable\n",
    "            pass\n",
    "        else:\n",
    "            # Make a new dict where the key is the key and the \n",
    "            # value is a list of the associated values of that \n",
    "            # key for every element (time step) of the initial list\n",
    "            key_dict = {\n",
    "                f\"{key}\": [input_ls[ticker][key] for ticker in range(len(input_ls))]\n",
    "            }\n",
    "            ticker += 1\n",
    "            attrs_dicts.append(key_dict)\n",
    "            keys_ls.append(key)\n",
    "\n",
    "    full_tuple = tuple(zip(keys_ls, attrs_dicts))\n",
    "\n",
    "    return full_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_tuple = stacked_meta_tuple(meta_attrs_list_vv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be a bit confusing. To help illustrate the above function, let's break down the output a bit more. \n",
    "\n",
    "`metadata_tuple` is a tuple of tuples. Each inner tuple holds a string and a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"({metadata_tuple[0][0]},{metadata_tuple[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO not sure how / why but every image in the time series is lsited as Sentinel-1 when I don't think this is possible with the repeat times? It's like that in the dir /file names as well as the README files so I don't think its anything I did processing wise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"({metadata_tuple[1][0]},{metadata_tuple[1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the beam mode of the images is 'Interferometric Wide'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Assign metadata tuple to a coordinate variable that exists along the time dimension of an Xarray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_meta_coords(input_xr, input_tuple):\n",
    "    \"\"\"takes an input xarray object and a tuple of metadata created from the above fn.\n",
    "    returns xr object with metadata from tuple applied\n",
    "    \"\"\"\n",
    "\n",
    "    out_xr = xr.Dataset(\n",
    "        input_xr.data_vars,\n",
    "        coords={\n",
    "            \"x\": input_xr.x.data,\n",
    "            \"y\": input_xr.y.data,\n",
    "        },\n",
    "    )\n",
    "    # now apply metadata coords\n",
    "    for element in range(len(input_tuple)):\n",
    "        key = input_tuple[element][0]\n",
    "        coord = list(input_tuple[element][1].values())[0]\n",
    "        out_xr.coords[f\"{key}\"] = (\"acq_date\", coord)\n",
    "\n",
    "    return out_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata = apply_meta_coords(ds, metadata_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, from a stack of files that make up a raster time series, we figured out how to take information stored in a every file name and assign it as non-dimensional coordinate variables of the data cube representing the raster time series. \n",
    "\n",
    "While it's good that we did this because there is nothing *ensuring* that the metadata in the file names is identical across time steps, many of these attributes are identical across the time series. This means that we can handle them more simply as attributes rather than coordinate variables, though we will still keep some metadata as coordinates. \n",
    "\n",
    "First, verify which metadata attributes are identical throughout the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make list of coords that are along time dimension\n",
    "coords_along_time_dim = [coord for coord in ds_w_metadata._coord_names if 'acq_date' in ds_w_metadata[coord].dims]\n",
    "dynamic_attrs = []\n",
    "static_attr_dict = {}\n",
    "for i in coords_along_time_dim:\n",
    "    if i != 'acq_date':\n",
    "        # find coordinate array variables that have more than\n",
    "        # one unique element along time dimension\n",
    "        if len(set(ds_w_metadata.coords[i].data.tolist())) > 1:\n",
    "            dynamic_attrs.append(i)\n",
    "        else:\n",
    "            static_attr_dict[i] = ds_w_metadata.coords[i].data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the static coordinates from the data cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata = ds_w_metadata.drop_vars(list(static_attr_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add them as attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata.attrs = static_attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Add metadata from README file\n",
    "\n",
    "Until now, all of the attribute information that we've added to the data cube came from file names of the individual scenes that make up the data cube. One piece of information not included in the file name is the original granule ID of the scene published by ESA. This is information about the source image, produced by ESA, and used by ASF when processing a SLC image into a RTC image. This is located in the README file within each scene directory. \n",
    "\n",
    "\n",
    "Having the granule ID can be helpful because it tells you if the satellite was in an ascending or a descending cycle when the image was captured. For surface conditions with diurnal fluctuations such as snowmelt, the timing of the image acquisition may be important.\n",
    "\n",
    ":::{note} \n",
    "This notebook works through the steps to format and organize metadata while explaining the details behind these operations. It is meant to be an educational demonstration. There are packages available such as [Sentinelsat](https://sentinelsat.readthedocs.io/en/stable/) that simplify metadata retrieval of Sentinel-1 imagery.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Extact granule ID\n",
    "\n",
    "We first need to extract the text containing a scene's granule ID from that scene's README file. This is similar to parsing the file and directory names but this time we're extracting text from a markdown file. To do this, we use the [python-markdown](https://python-markdown.github.io/) package. From looking at the file, we know where the source granule information is located; we need to isolate it from the rest of the text in the file. The source granule information always follows the same pattern, so we can use string methods to extract it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_granule_id(filepath):\n",
    "    ''' takes a filepath to the readme associated with an S1 scene and returns the source granule id used to generate the RTC imagery''' \n",
    "\n",
    "    # Use markdown package to read text from README\n",
    "    md = markdown.Markdown(extensions=['meta'])\n",
    "    # Extract text from file\n",
    "    data = pathlib.Path(filepath).read_text()\n",
    "    #this text precedes granule ID in readme\n",
    "    pre_gran_str = 'The source granule used to generate the products contained in this folder is:\\n'\n",
    "    split = data.split(pre_gran_str)\n",
    "    #isolate the granule id\n",
    "    gran_id = split[1][:67]\n",
    "    \n",
    "    return gran_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, take a look at a single source granule ID string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "granule_ls = [\n",
    "        extract_granule_id(filepaths_rm[element])\n",
    "        for element in range(len(filepaths_rm))\n",
    "    ]\n",
    "granule_ls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is information from the ESA's [SentiWiki](https://sentiwiki.copernicus.eu/web/) on [Sentinel-1 Product Naming Conventions](https://sentiwiki.copernicus.eu/web/s1-products#S1Products-SARNamingConventionS1-Products-SAR-Naming-Convention). In addition to the sensor and the beam mode, it tells us that the product type is a 'Single Look Complex' (SLC) image and is Level-1 processed data. The two date-time like sections indicate the start (14 February 2022, 12:13:53) and stop (14 February 2022, 12:14:20) date and time of the observation which are followed by the absolute orbit number (041909), a mission data take ID (04FD6F) and a unique product ID (42AF). \n",
    "\n",
    "```{image} ../imgs/esa_s1_naming.png\n",
    "align: center\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "TODO\n",
    "- probably save clipping until next notebook. write function in scripts that does all of this metadata wrangling under the hood for the next notebooks or save to disk i cant remember whcih is better. oh wait maybe just clip at hte end of this notebook and write to disk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Build granule ID coordinate array\n",
    "\n",
    "This is another situation of time-varying metadata. Once we have the indiviudal granule IDs, we must format them so that they can be attached to an Xarray object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coord_data(readme_fpaths_ls):\n",
    "    \"\"\"takes a list of the filepaths to every read me, extracts the granule ID.\n",
    "    From granule ID, extracts acquisition date and data take ID.\n",
    "    Returns a tuple of lists of acquisition dates and data take IDs.\"\"\"\n",
    "\n",
    "    #Make a list of all granules in time series\n",
    "    granule_ls = [\n",
    "        extract_granule_id(readme_fpaths_ls[element])\n",
    "        for element in range(len(readme_fpaths_ls))\n",
    "    ]\n",
    "    # Define a schema for aquisition date\n",
    "    schema = {\n",
    "        \"mission_identifier\": (3, r\"S1[A-B]\"),  # schema for sensor\n",
    "        \"mode_beam_identifier\": (2, r\"[A-Z]{2}\"),  # schema for beam mode\n",
    "        \"esa_product_type\": (3, r\"[A-Z]{3}\"),  # schema for ESA product type\n",
    "        \"proc_lvl_class_pol\": (4, f\"[A-Z0-9]{{4}}\"),  \n",
    "        \"acq_start\": (15, r\"[0-9]{8}T[0-9]{6}\"),  # schema for acquisition dat\n",
    "        \"acq_stop\" : (15, r\"[0-9]{8}T[0-9]{6}\"),  # schema for acquisition dat\n",
    "        \"orbit_no\" : (6, r\"[0-9]{6}\"),  # schema for orbit number\n",
    "        \"data_take_id\": (6, \"A-Z0-9{6}\"),  # schema for data take id\n",
    "\n",
    "    }\n",
    "    \n",
    "    #Extract relevant metadata from granule ID\n",
    "    all_granules_parsed_data = []\n",
    "    for granule in granule_ls:\n",
    "        #need to account for double under score\n",
    "        parts = [s for s in granule.split('_') if len(s) > 0]\n",
    "        #parts = granule.split(\"_\")\n",
    "        single_granule_parsed_data = {}\n",
    "        for part, (name, (length, pattern)) in zip(parts, schema.items()):\n",
    "            if name == 'acq_start': \n",
    "                single_granule_parsed_data['acq_start'] = pd.to_datetime(part, format=\"%Y%m%dT%H%M%S\")\n",
    "            elif name == 'data_take_id':\n",
    "                single_granule_parsed_data[name] = part\n",
    "        all_granules_parsed_data.append(single_granule_parsed_data)\n",
    "\n",
    "    acq_dates = [granule[\"acq_start\"] for granule in all_granules_parsed_data]\n",
    "    data_take_ids = [granule[\"data_take_id\"] for granule in all_granules_parsed_data]\n",
    "   \n",
    "    return (acq_dates, data_take_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_dates, data_take_ids_ls = make_coord_data(filepaths_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_da(variable_ls:list, variable:str, \n",
    "              acq_date_ls:list, desc:str):\n",
    "\n",
    "    da = xr.DataArray(\n",
    "        data = variable_ls,\n",
    "        dims=[\"acq_date\"],\n",
    "        coords = {\"acq_date\": acq_date_ls},\n",
    "        name = variable,\n",
    "        attrs= {\"description\": desc}\n",
    "    )\n",
    "    da = da.sortby(da.acq_date)\n",
    "    return da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_take_id_da = create_da(variable_ls = data_take_ids_ls, \n",
    "                       variable='data_take_id',\n",
    "                       acq_date_ls=acquisition_dates,\n",
    "                       desc='ESA Mission data take ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `xr.DataArray` of ESA data take IDs to raster data cube of Sentinel-1 scenes as a coordinate variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata.coords['data_take_ID'] = data_take_id_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle nodata values\n",
    "\n",
    "Another important difference between the `xr.open_mfdataset()` and VRT approaches is the handling of the nodata values. In the object built using `xr.open_mfdataset()`, nodata values are set to NaN while building the object from VRT files converts nodata values to zeros. This can cause a stream of problems later on so it is best to convert them to NaNs now. You can check the handling of nodata values in your dataset by using the `rio.nodata` accessor (shown below) which you can read about [here](https://corteva.github.io/rioxarray/stable/getting_started/nodata_management.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_w_metadata.vv.isel(acq_date=0).rio.nodata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clip by spatial area of interest\n",
    "\n",
    "Use a vector data frame to subset the large raster data cube to a smaller area of interest. In this case, the vector dataframe used is called `pc_aoi` because it is the spatial footprint used when accessing another version of the Sentinel-1 RTC dataseet processed by Planetary Comptuter (TODO clean up wording/refs and insert link to pc notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_aoi = gpd.read_file(\n",
    "    \"https://github.com/e-marshall/sentinel1_rtc/raw/main/hma_rtc_aoi.geojson\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
