{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a585e7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3.1 Accessing cloud-hosted ITS_LIVE data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to query and access cloud-hosted Inter-mission Time Series of Land Ice Velocity and Elevation ([ITS_LIVE](https://its-live.jpl.nasa.gov/#access)) data from Amazon Web Services (AWS) S3 buckets. These data are stored as [Zarr](https://zarr.readthedocs.io/en/stable/) data cubes, a cloud-optimized format for array data. They are read into memory as [Xarray](https://docs.xarray.dev/en/stable/) Datasets.\n",
    "\n",
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af38af9",
   "metadata": {
    "tags": []
   },
   "source": [
    "::::{tab-set}   \n",
    ":::{tab-item} Outline\n",
    "\n",
    "(content:section_A)= \n",
    "**[A. Overview of ITS_LIVE data](#a-overview-of-its_live-data)**\n",
    "- 1) Data structure overview\n",
    "\n",
    "(content:Section_B)=\n",
    "**[B. Read ITS_LIVE data from AWS S3 using Xarray](#b-read-its_live-data-from-aws-s3-using-xarray)**\n",
    "- 1) Overview of ITS_LIVE data storage and catalog\n",
    "- 2) Read ITS_LIVE data from S3 storage into memory\n",
    "- 3) Check spatial footprint of data\n",
    "\n",
    "(content:Section_C)=\n",
    "**[C. Query ITS_LIVE catalog](#c-query-its_live-catalog)**\n",
    "\n",
    "- 1) Find ITS_LIVE granule for a point of interest\n",
    "- 2) Read + visualize spatial footprint of ITS_LIVE data\n",
    ":::\n",
    "\n",
    ":::{tab-item} Learning Goals\n",
    "#### Concepts\n",
    "- Understand how data is organized in AWS S3 buckets,\n",
    "- Query and access cloud-optimized dataset from cloud object storage,\n",
    "- Create a vector data object representing the footprint of a raster dataset,\n",
    "- Preliminary visualization of data extent,\n",
    "  \n",
    "#### Techniques\n",
    "- Use [Xarray](https://xarray.dev/) to open [Zarr](https://zarr.readthedocs.io/en/stable/) data cubes stored in [AWS S3 bucket](https://aws.amazon.com/s3/),\n",
    "- Interactive data visualization with [hvplot](https://hvplot.holoviz.org/),\n",
    "- Create [Geopandas](https://geopandas.org/en/stable/) `geodataframe` from Xarray `xr.Dataset` object,\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad5a15",
   "metadata": {
    "tags": []
   },
   "source": [
    "Expand the next cell to see specific packages used in this notebook and relevant system and version information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc380c8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "169eebf3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Minimal\n"
     ]
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.1.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 9;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.0/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.0/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.0/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.1.1.min.js\", \"https://cdn.holoviz.org/panel/1.2.0/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%xmode minimal\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import xarray as xr\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73cf7d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A. Overview of ITS_LIVE data\n",
    "\n",
    "Skipping ahead a few steps, let's take a look at an ITS_LIVE data cube so that we have some expectations about what we'll see in the data catalog and once we read a data cube into memory. \n",
    "\n",
    "Specifically, we want to understand an ITS_LIVE time series data cube in the context of the Xarray data model. If you're new to working with Xarray, the [Data Structures](https://docs.xarray.dev/en/latest/user-guide/data-structures.html) documentation is very useful for getting a hang of the different components that are the building blocks of `Xarray.Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "829dc0c5",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "init_url = \"http://its-live-data.s3.amazonaws.com/datacubes/v2-updated-october2024/N60W130/ITS_LIVE_vel_EPSG3413_G0120_X-3250000_Y250000.zarr\"\n",
    "datacube = xr.open_dataset(init_url, engine=\"zarr\", decode_timedelta=True, chunks=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28de315a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunks': (100426,),\n",
       " 'preferred_chunks': {'mid_date': 100426},\n",
       " 'compressor': Blosc(cname='lz4', clevel=5, shuffle=SHUFFLE, blocksize=0),\n",
       " 'filters': None,\n",
       " 'dtype': dtype('<U2')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacube['satellite_img1'].encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb514f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1) Data structure overview\n",
    "\n",
    "#### Dimensions\n",
    "- This object has 3 *dimensions*, `mid_date`, `x`, and `y`.\n",
    "- Each dimension has a corresponding coordinate variable of the same name. Think of these as \"axis ticks\" on a figure if you were to plot the data.\n",
    "\n",
    "#### Data Variables\n",
    "- Expanding the 'Data Variables' label, you can see that there are many (60!) variables.\n",
    "- Each variable exists along one or more dimension (eg. `(mid_date,x,y)`), has an associated data type (eg.`float32`), and has an underlying array that holds that variable's data. \n",
    "\n",
    "#### Attributes\n",
    "- Data is commonly associated with related \"metadata\" -- data that describes data. For example, the `floatingice` variable has an attribute `description : floating ice mask, 0 = non-floating-ice, 1 = floating-ice` that tells you how to interpret its values. All array-based Xarray objects (data variables, coordinate variables, DataArrays and Datasets) can have attributes attached to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e16cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datacube.floatingice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4f996",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Other Coordinate Variables\n",
    "\n",
    "Metadata can take the form of dimensional arrays too. For example, the `satellite_img1` and `satellite_img2` arrays record the satellite sources for the image pair used to construct the velocity data. This is important *metadata* about the observed velocity fields. Such variables can be set as \"non-dimension coordinate variables\" if desired, though we will not do so here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5609f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datacube.satellite_img1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ef2983",
   "metadata": {
    "tags": []
   },
   "source": [
    ":::{tip}\n",
    "If you haven't yet, review the {term}`Metadata naming` and {term}`Climate Forecast (CF) Metadata Conventions` sections of the [Relevant Concepts](../../background/6_relevant_concepts.md) page.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82771806",
   "metadata": {
    "tags": []
   },
   "source": [
    "## B. Read ITS_LIVE data from AWS S3 using Xarray\n",
    "\n",
    "Now that we know a bit more about the ITS_LIVE dataset, we can start querying the catalog to access the data we're interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f46458b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1) Overview of ITS_LIVE data storage and catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e3f13",
   "metadata": {
    "tags": []
   },
   "source": [
    "The ITS_LIVE project details a number of data access options on their [website](https://its-live.jpl.nasa.gov/#access). Here, we will be accessing ITS_LIVE data in the form of [zarr](https://zarr.readthedocs.io/en/stable/) data cubes that are stored in [S3 buckets](https://registry.opendata.aws/its-live-data/) hosted by Amazon Web Services (AWS). There is a [AWS S3 explorer index](https://its-live-data.s3.amazonaws.com/index.html) that we will use to query the data catalog. There, you can browse the contents of the bucket in the AWS S3 Explorer. Click this [link](https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json) to download the file.\n",
    "\n",
    ":::{tip}\n",
    "You can also use the ITS_LIVE API to access ITS_LIVE data cube urls corresponding to different search conditions as well as Python code provided on the ITS_LIVE [website](https://its-live.jpl.nasa.gov/#api). We go through the steps of looking at the catalog in order to get a better understanding of how S3 buckets are organized. \n",
    ":::\n",
    "\n",
    "To query the data stored in the bucket, we will download the `catalog_v02.json` that is located in the bucket linked above. \n",
    "\n",
    "#### Understanding the data\n",
    "\n",
    "The first step in working with a new dataset is understanding how it is organized. To query the data stored in the bucket, we will download the `catalog_v02.json` that is linked above. This catalog contains spatial information and properties of ITS_LIVE data cubes as well as the URL used to access each cube. Let's take a look at the entry for a single data cube and the information that it contains:\n",
    "\n",
    "```{image} ../imgs/screengrab_itslive_catalog_entry.png\n",
    ":center-align\n",
    "```\n",
    "\n",
    "The top portion of the picture shows the spatial extent of the data cube in lat/lon units. Below that, we have properties such as the [EPSG code of the coordinate reference system](https://en.wikipedia.org/wiki/EPSG_Geodetic_Parameter_Dataset), the spatial footprint in projected units, and the url of the zarr object. \n",
    "\n",
    "Let's take a look at the url more in-depth: \n",
    "```\n",
    "http://its-live-data.s3.amazonaws.com/datacubes/v2-updated-october2024/S40E170/ITS_LIVE_vel_EPSG32759_G0120_X450000_Y5250000.zarr\n",
    "```\n",
    "\n",
    "From this link we can see that we are looking at ITS_LIVE data located in an s3 bucket hosted by Amazon Web Services (AWS). We also see that we're looking in the version 2 data cube directory. The next bit gives us information about the global location of the cube (N40E080). The actual file name `ITS_LIVE_vel_EPSG32645_G0120_X250000_Y4750000.zarr` tells us that we are looking at ice velocity data (its_live also has elevation data), in the CRS associated with EPSG 32645 (this code indicates UTM zone 45N). X250000_Y4750000 tells us more about the spatial footprint of the datacube within the UTM zone. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1535a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2) Read ITS_LIVE data from S3 storage into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613de52",
   "metadata": {
    "tags": []
   },
   "source": [
    "We've found the url associated with the tile we want to access, let's try to open the data cube using `Xarray.open_dataset()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567cc3e",
   "metadata": {
    "tags": [
     "raises-exception",
     "output-scroll"
    ]
   },
   "outputs": [],
   "source": [
    "url = \"http://its-live-data.s3.amazonaws.com/datacubes/v2/N30E090/ITS_LIVE_vel_EPSG32646_G0120_X750000_Y3350000.zarr\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3fc72",
   "metadata": {
    "tags": []
   },
   "source": [
    "In addition to passing `url` to `xr.open_dataset()`, we include `chunks='auto'`. This introduces [dask](https://www.dask.org/) into our workflow; `chunks='auto'` will choose chunk sizes that match the underlying data structure; this is often ideal, but sometimes you may need to specify different chunking schemes. You can read more about choosing good chunk sizes [here](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes); subsequent notebooks in this tutorial will explore different approaches to dask chunking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814eb1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc = xr.open_dataset(url, decode_timedelta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb59153",
   "metadata": {
    "tags": []
   },
   "source": [
    "As you can see, this doesn’t quite work. When passing the url to `xr.open_dataset()`, if a backend isn’t specified, Xarray will expect a NetCDF file. Because we’re trying to open a Zarr file we need to add an additional argument to `xr.open_dataset()`, shown in the next code cell. You can find more information [here](https://docs.xarray.dev/en/stable/user-guide/io.html#cloud-storage-buckets). Another approach we could use is to read the data with the Zarr-specific method [`xr.open_zarr()`](https://docs.xarray.dev/en/stable/generated/xarray.open_zarr.html) instead of [`xr.open_dataset()`](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html).\n",
    "\n",
    "We set `decode_coords=\"all\"` so that Xarray will auto-detect a number of variables as coordinate variables --- these are variables that are usually describing properties that are common to many \"data variables\". In our case, it picks up the `mapping` variable which describes the Coordinate Reference System for this datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c159547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc = xr.open_dataset(url, engine=\"zarr\", chunks=\"auto\", decode_timedelta=False, decode_coords=\"all\")\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430de7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "This one worked! Let's stop here and define a function that we can use to read additional s3 objects into memory as Xarray Datasets. This will come in handy later in this notebook and in subsequent notebooks. We will store this and other utility functions in `itslive_tools.py` for reuse across notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04828f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_in_s3(http_url: str, chunks: str | dict | None = \"auto\") -> xr.Dataset:\n",
    "    \"\"\"I'm a function that takes a url pointing to the location of a zarr data cube.\n",
    "    I return an Xarray Dataset. I can take an optional chunk argument which specifies\n",
    "    how the data will be chunked when read into memory\"\"\"\n",
    "    datacube = xr.open_dataset(\n",
    "        http_url,\n",
    "        engine=\"zarr\",\n",
    "        chunks=chunks,\n",
    "        decode_coords=\"all\",\n",
    "        decode_timedelta=False,\n",
    "    )\n",
    "    return datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc28236",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3) Check spatial footprint of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377860e",
   "metadata": {
    "tags": []
   },
   "source": [
    "We just read in a very large dataset. \n",
    "\n",
    "We'd like an easy way to be able to visualize the footprint of this data to ensure we specified the correct location without plotting a data variable over the entire footprint, which would be much more computationally and time-intensive. \n",
    "\n",
    "To do so, we need to understand the coordinate system of the data, and its bounds.\n",
    "\n",
    "This dataset has its coordinate system info stored in an array named `mapping`. How would you know that? Scroll through the Xarray Dataset repr, and check the attributes. Variables with CRS information tend to have the `crs_wkt`, `grid_mapping`, `GeoTransform` and related attributes that describe the coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbae7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc.mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5b292",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following function creates a `GeoPandas.GeoDataFrame` describing the spatial footprint of an `xr.Dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903e129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bounds_polygon(input_xr: xr.Dataset) -> gpd.GeoDataFrame:\n",
    "    \"\"\"I'm a function that takes an Xarray Dataset and returns a GeoPandas DataFrame of the bounding box of the Xarray Dataset.\"\"\"\n",
    "\n",
    "    xmin = input_xr.coords[\"x\"].data.min()\n",
    "    xmax = input_xr.coords[\"x\"].data.max()\n",
    "\n",
    "    ymin = input_xr.coords[\"y\"].data.min()\n",
    "    ymax = input_xr.coords[\"y\"].data.max()\n",
    "\n",
    "    pts_ls = [(xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax), (xmin, ymin)]\n",
    "\n",
    "    crs = f\"epsg:{input_xr.mapping.spatial_epsg}\"\n",
    "\n",
    "    polygon_geom = Polygon(pts_ls)\n",
    "    polygon = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom])\n",
    "\n",
    "    return polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690fbfd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's take a look at the cube we've already read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b09635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox = get_bounds_polygon(dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c01c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "`get_bounds_polygon()` returns a geopandas.GeoDataFrame object in the same projection as the velocity data object (local UTM). Re-project to latitude/longitude to view the object more easily on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04184c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox = bbox.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9037c",
   "metadata": {
    "tags": []
   },
   "source": [
    "To visualize the footprint, we use the interactive plotting library, [hvPlot](https://hvplot.holoviz.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880832a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poly = bbox.hvplot(legend=True, alpha=0.3, tiles=\"ESRI\", color=\"red\", geo=True)\n",
    "poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed403e9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## C. Query ITS_LIVE catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc451d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1) Find ITS_LIVE granule for a point of interest\n",
    "Let's look in a different region and see how we could search the ITS_LIVE data cube catalog for the granule that covers our location of interest. There are many ways to do this, this is just one example. \n",
    "\n",
    "First, we read in the catalog GeoJSON file with geopandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e85b2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "itslive_catalog = gpd.read_file(\"https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json\")\n",
    "itslive_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59378266",
   "metadata": {
    "tags": []
   },
   "source": [
    "Below is a function to query the catalog for the s3 url covering a given point. You could easily tweak this function (or write your own!) to select granules based on different properties. Play around with the `itslive_catalog` object to become more familiar with the data it contains and different options for indexing.\n",
    "\n",
    ":::{note}\n",
    "Since this tutorial was originally written, the [ITS_LIVE Python Client](https://github.com/nasa-jpl/itslive-py) was released. This is a great way to access ITS_LIVE data cubes. \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c13e04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_granule_by_point(input_point: list) -> str:\n",
    "    \"\"\"I take a point in [lon, lat] format and return the url of the granule containing specified point.\n",
    "    Point must be passed in EPSG:4326.\"\"\"\n",
    "\n",
    "    catalog = gpd.read_file(\"https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json\")\n",
    "\n",
    "    # make shapely point of input point\n",
    "    p = gpd.GeoSeries([Point(input_point[0], input_point[1])], crs=\"EPSG:4326\")\n",
    "    # make gdf of point\n",
    "    gdf = gdf = gpd.GeoDataFrame({\"label\": \"point\", \"geometry\": p})\n",
    "    # find row of granule\n",
    "    granule = catalog.sjoin(gdf, how=\"inner\")\n",
    "\n",
    "    url = granule[\"zarr_url\"].values[0]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22b36f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Choose a location in Alaska:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983afde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = find_granule_by_point([-138.958776, 60.748561])\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0e3d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Great, this function returned a single url corresponding to the data cube covering the point we supplied. Let's use the `read_in_s3` function we defined to open the datacube as an `xarray.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44343b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datacube = read_in_s3(url)\n",
    "datacube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf8c6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2) Read + visualize spatial footprint of ITS_LIVE data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc2be3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Use the `get_bounds_polygon()` function to take a look at the footprint using `hvplot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e843b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox_dc = get_bounds_polygon(datacube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd503c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poly = bbox_dc.to_crs(\"EPSG:4326\").hvplot(legend=True, alpha=0.5, tiles=\"ESRI\", color=\"red\", geo=True)\n",
    "poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09380b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "This notebook demonstrated how to query and access a cloud-optimized remote sensing time series dataset stored in an AWS S3 bucket. The subsequent notebooks in this tutorial will go into much more detail on how to organize, examine and analyze this data. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.188405,
   "end_time": "2025-03-22T05:49:05.536366",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/emmamarshall/Desktop/phd_research/ch1/cloud_os_geospatial_datacube_workflows/book/itslive/nbs/1_accessing_itslive_s3_data.ipynb",
   "output_path": "/home/emmamarshall/Desktop/phd_research/ch1/cloud_os_geospatial_datacube_workflows/book/itslive/nbs/1_accessing_itslive_s3_data.ipynb",
   "parameters": {},
   "start_time": "2025-03-22T05:48:51.347961",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
