{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e79c758-1b29-4abd-b288-735f379d0284",
   "metadata": {},
   "source": [
    "{{title_its_nb5}}\n",
    "\n",
    "{{intro}}\n",
    "\n",
    "The previous notebooks in this tutorial demonstrated how to use Xarray to access, inspect, manipulate and analyze raster time series data at the scale of an individual glacier. In this notebook, we shift our focus to a sub-regional scale, looking at all of the glaciers within a given ITS_LIVE data cube. This workflow will draw on elements from the past notebooks while introducing new tools for examining raster data along temporal and spatial dimensions (ie. across multiple glaciers). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df8f98",
   "metadata": {},
   "source": [
    "::::{tab-set} \n",
    ":::{tab-item} Outline\n",
    "\n",
    "(content:Section_A)=\n",
    "**[A. Read and organize data](#a-read-and-organize-data)**\n",
    "- {{a1_its_nb5}}\n",
    "- {{a2_its_nb5}}\n",
    "\n",
    "(content:Section_B)=\n",
    "**[B. Combine raster and vector data to create a vector data cube](#b-combine-raster-and-vector-data-to-create-a-vector-data-cube)**\n",
    "- {{b1_its_nb5}}\n",
    "- {{b2_its_nb5}}\n",
    "- {{b3_its_nb5}}\n",
    "\n",
    "(content:Section_C)= \n",
    "**[C. Data visualization](#c-data-visualization)**\n",
    "- {{c1_its_nb5}}\n",
    "- {{c2_its_nb5}}\n",
    "- {{c3_its_nb5}}\n",
    ":::  \n",
    ":::{tab-item} Learning Goals\n",
    "{{concepts}}\n",
    "- Querying and accessing raster data from cloud object storage\n",
    "- Accessing and manipulating vector data\n",
    "- Handling coordinate reference information\n",
    "- Calculating and visualizing summary statistics\n",
    "\n",
    "{{techniques}}\n",
    "- Access cloud-hosted [Zarr](https://zarr.readthedocs.io/en/stable/) data cubes using [Xarray](https://xarray.dev/)\n",
    "- Reading [GeoParquet](https://geoparquet.org/) vector data using [GeoPandas](https://geopandas.org/en/stable/)\n",
    "- Rasterize vector objects using [Geocube]()\n",
    "- Spatial joins of vector datasets using [GeoPandas](https://geopandas.org/en/stable/)\n",
    "- Using [dask](https://www.dask.org/) to work with out-of-memory data\n",
    "- Calculating summary statistics of [Xarray](https://xarray.dev/) and [Pandas](https://pandas.pydata.org/) objects\n",
    "- Data visualization using [Pandas](https://pandas.pydata.org/)\n",
    "- Interactive data visualization with [GeoPandas](https://geopandas.org/en/stable/)\n",
    ":::\n",
    "::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011f97f",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0705af",
   "metadata": {},
   "source": [
    "Expand the next cell to see specific packages used in this notebook and relevant system and version information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c040afe-7f0f-4103-b9c7-02f38e189de2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%xmode minimal\n",
    "\n",
    "import contextily as cx\n",
    "import dask\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import xarray as xr\n",
    "import xvec\n",
    "\n",
    "import itslivetools\n",
    "\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "tutorial1_dir = pathlib.Path(cwd).parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddabd89",
   "metadata": {},
   "source": [
    "{{break}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a61c98-901e-425d-ad6c-4627b92bf6df",
   "metadata": {},
   "source": [
    "## A. Read and organize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c699cdd",
   "metadata": {},
   "source": [
    "### {{a1_its_nb5}}\n",
    "\n",
    "As in past notebooks, we use `itslivetools.find_granule_by_point()` to query the ITS_LIVE catalog for the correct url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4942a00-751c-4ffa-b8c5-56d0ae07fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "itslive_catalog = gpd.read_file(\n",
    "    \"https://its-live-data.s3.amazonaws.com/datacubes/catalog_v02.json\"\n",
    ")\n",
    "url = itslivetools.find_granule_by_point([95.180191, 30.645973])\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0511d11",
   "metadata": {},
   "source": [
    "Following the example shown in the initial data inspection [notebook](), we read the data in *without* dask at first so that we can lazily organize the dataset in chronological order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56626a-13cc-4823-a8de-21293366e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = itslivetools.read_in_s3(url, chunks=None)\n",
    "dc = dc.sortby(\"mid_date\")\n",
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1314a32d",
   "metadata": {},
   "source": [
    "Now we add a chunking scheme, converting the underlying Numpy arrays to Dask arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check the preffered chunk sizes\n",
    "dc[\"v\"].encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e441260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, chunk dataset\n",
    "dc = dc.chunk({\"mid_date\": 20000, \"x\": 10, \"y\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452444f",
   "metadata": {},
   "source": [
    "We'll resample the time series to 3-month resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb4da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_resamp = dc.resample(mid_date=\"3ME\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33c0bc-45ac-4eba-9f33-3860989b410a",
   "metadata": {},
   "source": [
    "Create a `crs` object based on the `projection` data variable of the data cube (`dc`) object. We'll use this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1957308",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = f\"EPSG:{dc.projection}\"\n",
    "crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100f674",
   "metadata": {},
   "source": [
    "### {{a2_its_nb5}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb739146",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia = gpd.read_parquet(\"../data/rgi7_region15_south_asia_east.parquet\")\n",
    "\n",
    "se_asia.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aaa855",
   "metadata": {},
   "source": [
    "What coordinate reference system is this dataframe in? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616aad9",
   "metadata": {},
   "source": [
    "The vector dataset is in WGS 84, meaning that its coordinates are in degrees latitude and longitude rather than meters N and E. We will project this dataset to match the projection of the raster dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd69a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_asia_prj = se_asia.to_crs(crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be04121",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(se_asia_prj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ccab6",
   "metadata": {},
   "source": [
    "The vector dataframe representing glacier outlines is very large. For now, we're only interested in glaciers that lie within the footprint of the ITS_LIVE granule we're working with and that are larger than 5 square kilometers in area. We subset the full dataset to match those conditions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e1db7",
   "metadata": {},
   "source": [
    "To start with, we will look only at glaciers larger in area than 5km2. Subset the dataset to select for those glaciers; start by making a GeoDataFrame of the ITS_LIVE granule footprint in order to perform a spatial join and select only the glaciers from the RGI dataframe (`se_asia_prj`) that are within the granule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37523806",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_bbox = itslivetools.get_bounds_polygon(dc)\n",
    "dc_bbox[\"Label\"] = [\"Footprint of ITS_LIVE granule\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24098619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial join\n",
    "rgi_subset = gpd.sjoin(se_asia_prj, dc_bbox, how=\"inner\")\n",
    "# Select only glaciers where area >= 5 km2\n",
    "rgi_subset = rgi_subset.loc[rgi_subset[\"area_km2\"] >= 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee71a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Now, we are looking at {len(rgi_subset)} glaciers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68851c86",
   "metadata": {},
   "source": [
    "## B. Combine raster and vector data to create a vector data cube\n",
    "\n",
    "Vector data cubes are a data structure similar to a raster data cube, but with a dimension represented by an array of geometries. For a detailed explanation of vector data cubes, see Edzer Pebesma's [write-up](https://r-spatial.org/r/2022/09/12/vdc.html). [Xvec](https://xvec.readthedocs.io/en/stable/) is a relatively new Python package that implements vector data cubes within the Xarray ecosystem. This is an exciting development that can drastically simplify workflows that examine data along both spatial and temporal dimensions and involve spatial features represented by vector points, lines and polygons.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf5132",
   "metadata": {},
   "source": [
    "To explain this in more detail, we currently have a raster data cube (the ITS_LIVE time series) that covers the entire spatial footprint shown in blue below. However, the locations in which we are interested in this data are the glaciers outlined in red. Working with this data as a 3-dimensional vector cube is not a very efficient way of accessing the data at the locations shown in red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753d2ad",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "dc_bbox.to_crs(\"EPSG:3857\").plot(ax=ax, alpha=0.5)\n",
    "rgi_subset.to_crs(\"EPSG:3857\").plot(ax=ax, facecolor=\"none\", edgecolor=\"r\")\n",
    "\n",
    "cx.add_basemap(\n",
    "    ax=ax,\n",
    "    crs=\"EPSG:3857\",\n",
    "    source=cx.providers.Esri.WorldImagery,\n",
    ")\n",
    "fig.suptitle(\n",
    "    \"Spatial extent of ITS_LIVE granule, shown in blue \\n outlines of glaciers of interest, shown in red.\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144bbefb",
   "metadata": {},
   "source": [
    "Instead, we use the [`Xvec.zonal_stats()`](https://xvec.readthedocs.io/en/stable/zonal_stats.html) method to convert the 3-dimensional cube to a 2-dimensional cube that has  time dimesnion and a geometry dimension. Each element of the geometry dimension is a glacier from the `rgi_subset` dataframe.\n",
    "\n",
    "```{note} \n",
    "Because we are working with polygon geometries, we use zonal_stats() which performs a reduction over the area of the polygon. If our vector data was made up of point features, we could use [`Xvec.extract_points()`](https://xvec.readthedocs.io/en/stable/extract_pts.html)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edd73d",
   "metadata": {},
   "source": [
    "### {{b1_its_nb5}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5c1f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "vector_data_cube = dc_resamp.xvec.zonal_stats(\n",
    "    rgi_subset.geometry,\n",
    "    x_coords=\"x\",\n",
    "    y_coords=\"y\",\n",
    ").drop_vars(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5084238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd74b3",
   "metadata": {},
   "source": [
    "Great, now we've gone from a 3-d object with (mid_date,x,y) dimensions to a 2-d object with (mid_date, geometry) dimensions. However, in addition to the geometry data stored in the vector dataframe, we'd also like to add some of the attribute data to the ITS_LIVE time series vector cube. The following cell adds attributes as coordinate variables to the vector data cube."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a884c58",
   "metadata": {},
   "source": [
    "### {{b2_its_nb5}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32948f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define attributes to be added\n",
    "rgi_attrs_dict = {\"RGIId\": \"rgi_id\", \"Area_km2\": \"area_km2\", \"Slope_deg\": \"slope_deg\"}\n",
    "\n",
    "\n",
    "def update_cube_attrs(ds, gdf, attrs_dict):\n",
    "    for k, v in attrs_dict.items():\n",
    "        ds[k] = ((\"geometry\"), gdf[v].values)\n",
    "        ds = ds.assign_coords({k: ds[k]})\n",
    "    return ds\n",
    "\n",
    "\n",
    "vector_data_cube = update_cube_attrs(vector_data_cube, rgi_subset, rgi_attrs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76724be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a2c51",
   "metadata": {},
   "source": [
    "### {{b3_its_nb5}}\n",
    "\n",
    "Later in this notebook, we will want to perform operations that require this object to be loaded into memory. All of our steps thus far have happened lazily, and when we do load the data into memory it will be quite time-consuming. \n",
    "\n",
    "To skip this step, I've loaded the data into memory and written it to disk. This allows us to read the vector data cube (which is much more manageable) into memory from file. \n",
    "\n",
    "Because this step is quite computationally intensive, I won't execute it here, but I will include the code for the sake of completeness. \n",
    "\n",
    "#### Steps to write vector data cube to disk\n",
    "1. Encode vector data code geometries as [CF] geometries. \n",
    "This is required because Shapely geometries (how geometries are represented in memory) are not compatible with array-based file formats such as Zarr. To get around this, Xvec uses a package called cf-xarray to encode the Shapely geometries as CF geometries. For more detail on reading and writing vector data cubes, see the Xvec [documentation](https://xvec.readthedocs.io/en/stable/io.html).\n",
    "\n",
    "    ```python\n",
    "    encoded_vdc = vector_data_cube.xvec.encode_cf()\n",
    "    ```\n",
    "\n",
    "2. Write encoded data cube to Zarr (*This triggers `.compute()`, loading the data into memory before writing it to disk.* On my computer, this took about 12 minutes. \n",
    "    ```python\n",
    "    with ProgressBar():\n",
    "\n",
    "        encoded_vdc.to_zarr('../data/regional_glacier_velocity_vector_cube.zarr', mode='w')\n",
    "    ```\n",
    "    Output:\n",
    "    [########################################] | 100% Completed | 12m 15s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49360a3d",
   "metadata": {},
   "source": [
    "## C. Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336adca0",
   "metadata": {},
   "source": [
    "### {{c1_its_nb5}}\n",
    "\n",
    "The vector data cube is stored on disk with CF geometries. We go through the reverse of the process we used to write the object to disk, decoding the CF geometries to Shapely geometries with `xvec.decode_cf()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0672220",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data_cube_cf = xr.open_zarr(\n",
    "    os.path.join(tutorial1_dir, \"data/regional_glacier_velocity_vector_cube.zarr\")\n",
    ")\n",
    "vector_data_cube = vector_data_cube_cf.xvec.decode_cf().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9de829",
   "metadata": {},
   "source": [
    "### {{c2_its_nb5}}\n",
    "\n",
    "Xvec has a method, `to_geodataframe()` that allows us to easily convert the `xr.Dataset` vector cube to a `gpd.GeoDataFrame`. We can then use the GeoPandas `.explore()` method to interactively visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc52e5cd",
   "metadata": {},
   "source": [
    "We will look at mean velocities over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data_cube_mean = vector_data_cube.mean(dim=\"mid_date\")\n",
    "\n",
    "vector_data_cube_mean[\"vmag\"] = np.sqrt(\n",
    "    vector_data_cube_mean[\"vx\"] ** 2 + vector_data_cube_mean[\"vy\"] ** 2\n",
    ")\n",
    "\n",
    "vector_data_cube_mean[\"vmag\"].xvec.to_geodataframe(geometry=\"geometry\").explore(\"vmag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44d9807",
   "metadata": {},
   "source": [
    "We could also look at single seasons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_cube_seasonal_mean = vector_data_cube.groupby(\"mid_date.season\").mean()\n",
    "\n",
    "vector_cube_seasonal_mean[\"vmag\"] = np.sqrt(\n",
    "    vector_cube_seasonal_mean[\"vx\"] ** 2 + vector_cube_seasonal_mean[\"vy\"] ** 2\n",
    ")\n",
    "\n",
    "vector_cube_seasonal_mean.sel(season=\"JJA\")[\"vmag\"].xvec.to_geodataframe(\n",
    "    geometry=\"geometry\"\n",
    ").explore(\"vmag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f23e78-9f70-40cf-ad40-69d7e18d57b8",
   "metadata": {},
   "source": [
    "### {{c3_its_nb5}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e8adb-3b43-466a-8ec7-c0887e27bc52",
   "metadata": {},
   "source": [
    "In addition to using GeoPandas plotting features, we can still use Xarray's built in plotting features to visualize the non-spatial elements of the data. Below, we visualize the relationships between magnitude of velocity (y-axis), glacier slope (x-axis) and glacier area (hue). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.suptitle(\"Mean velocity in the context of glacier area and slope\", fontsize=16)\n",
    "\n",
    "sc = vector_data_cube_mean.plot.scatter(\n",
    "    x=\"Slope_deg\",\n",
    "    y=\"vmag\",\n",
    "    hue=\"Area_km2\",\n",
    "    ax=ax,\n",
    "    add_colorbar=True,\n",
    "    cbar_kwargs={\"label\": \"Glacier area (sq. km.)\"},\n",
    ")\n",
    "ax.set_ylabel(\"Magnitude of velocity (m/yr)\", fontsize=14)\n",
    "ax.set_xlabel(\"Glacier slope (degrees)\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d37a23",
   "metadata": {},
   "source": [
    "{{conclusion}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
