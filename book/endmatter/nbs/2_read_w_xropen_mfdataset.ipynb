{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626f0868-f402-4377-b50d-1eb7c4a4b2da",
   "metadata": {},
   "source": [
    "# Read Sentinel-1 RTC data processed by ASF using `xr.open_mfdataset()`\n",
    "\n",
    "This notebook demonstrates working with Sentinel-1 RTC imagery that has been processed on the ASF On-Demand server and downloaded locally. \n",
    "\n",
    "The access point for data in this notebook is a directory containing un-zipped directories of RTC scenes.\n",
    "\n",
    "This notebook will detail how to use the xarray function `xr.open_mfdataset()`. While this approach works and is very useful for working with large stacks of data with associated metadata, you will also see an example of the limitations of this approach due to certain characteristics of the dataset. You can read more about `xr.open_mfdataset()` [here](https://docs.xarray.dev/en/stable/generated/xarray.open_mfdataset.html).\n",
    "\n",
    "**Learning goals**:\n",
    "\n",
    "Handling large amounts of locally stored data\n",
    "- organize large set of geotiff files stored locally\n",
    "- multiple methods for reading data into python as `xarray` objects\n",
    "    - xarray `open_mfdataset()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04cd65f-4a6b-4f66-98ba-8b705a7e642a",
   "metadata": {},
   "source": [
    "## Software and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb32b85-47b7-4e1d-baeb-48af7116ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import re\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa7cdb-2e90-4fc3-a944-84dcfdf6953e",
   "metadata": {},
   "source": [
    "We'll use this function later but define it now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a39f0c9-af87-4f06-8245-96736ac5d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points2coords(pt_ls):  # should be [xmin, ymin, xmax, ymax]\n",
    "    coords_ls = [\n",
    "        (pt_ls[0], pt_ls[1]),\n",
    "        (pt_ls[0], pt_ls[3]),\n",
    "        (pt_ls[2], pt_ls[3]),\n",
    "        (pt_ls[2], pt_ls[1]),\n",
    "        (pt_ls[0], pt_ls[1]),\n",
    "    ]\n",
    "    return coords_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e581e-2305-4b1c-865e-75d7cea30f35",
   "metadata": {},
   "source": [
    "Initialize a `dask.distributed` client:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbbb86-41d9-4e71-9ecc-3cfbc35d01ff",
   "metadata": {},
   "source": [
    "```{note} \n",
    "On my local machine, I ran into issues initially when the cluster was running on processes rather than threads. This caused system memory issues and many dead kernels. Setting `processes=False` seemed to fix these issues\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7c0ee-eb3d-42a4-bb00-c4cb8b11c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(processes=False)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a906cc1-1d1e-4d6e-9775-46dfdc6c6ad5",
   "metadata": {},
   "source": [
    "Open the dask dashboard at the link above to monitor task progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48327c7f-982a-46e7-b1f9-39d527d66b8f",
   "metadata": {},
   "source": [
    "### Organize file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d128b-1696-4eba-a8a6-b89b08ea2764",
   "metadata": {},
   "source": [
    "Set up some string variables for directory paths. We need to pass `xr.open_mfdataset()` a list of all files to read in. Currently, the file structure is organized so that each scene has its own sub-directory within `asf_rtcs`. Within each sub-directory are several files - we want to extract the GeoTIFF files containing RTC imagery for the VV and VH polarizations for each scene as well as the layover-shadow mask for each scene. The function `extract_tif_fnames()` takes a path to the directory containing the sub-directories for all scenes and returns a list of the filenames for VV-polarization GeoTIFF files, VH-polarization GeoTIFF files and layover-shadow mask GeoTIFF files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8f65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path.cwd()\n",
    "tutorial2_dir = pathlib.Path(cwd).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2fc9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_type = \"full\"\n",
    "path_to_rtcs = f\"tutorial2/data/{timeseries_type}_timeseries/asf_rtcs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c996488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(str(cwd.parents[1]), path_to_rtcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d0d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to directory holding downloaded data\n",
    "s1_asf_data = os.path.join(str(cwd.parents[1]), path_to_rtcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b948a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_ls = os.listdir(s1_asf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16fe6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fnames(data_path: str, scene_name:str, variable:str):\n",
    "    # Make list of files within each scene directory in data directory\n",
    "    scene_files_ls = os.listdir(os.path.join(data_path, scene_name))\n",
    "\n",
    "    if variable in ['vv','vh']:\n",
    "        scene_files = [fname for fname in scene_files_ls if fname.endswith(f\"_{variable.upper()}.tif\")]\n",
    "    \n",
    "    elif variable == 'ls_map':\n",
    "        scene_files = [fname for fname in scene_files_ls if fname.endswith(f\"_ls_map.tif\")]\n",
    "    \n",
    "    elif variable == 'readme':\n",
    "        scene_files = [file for file in scene_files_ls if file.endswith(\"README.md.txt\")]\n",
    "   \n",
    "    return scene_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_fnames(s1_asf_data, scenes_ls[0], 'vv'))\n",
    "print(extract_fnames(s1_asf_data, scenes_ls[0], 'vh'))\n",
    "print(extract_fnames(s1_asf_data, scenes_ls[0], 'ls_map'))\n",
    "print(extract_fnames(s1_asf_data, scenes_ls[0], 'readme'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff41701-8f7e-4f22-839f-47260a04419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filepath_lists(asf_s1_data_path: str, variable:str):\n",
    "    \"\"\" For a single variable (vv, vh, ls_map or readme), make list of \n",
    "    full filepath for each file in time series. Also return dates to ensure\n",
    "    extraction happens in correct order for each variable\n",
    "    Return tuple with form (filepaths list, dates list)\"\"\"\n",
    "    scenes_ls = os.listdir(asf_s1_data_path)\n",
    "\n",
    "    fpaths, dates_ls = [],[]\n",
    "\n",
    "    for element in range(len(scenes_ls)):\n",
    "\n",
    "        # Extract filenames of each file of interest\n",
    "        files_of_interest = extract_fnames(asf_s1_data_path, scenes_ls[element], variable)\n",
    "        # Make full path with filename for each variable\n",
    "        path = os.path.join(\n",
    "            asf_s1_data_path, scenes_ls[element], files_of_interest[0]\n",
    "        )\n",
    "        #extract dates to make sure dates are identical across variable lists\n",
    "        date = pathlib.Path(path).stem.split(\"_\")[2]\n",
    "        \n",
    "        dates_ls.append(date)\n",
    "        fpaths.append(path)\n",
    "    \n",
    "    return (fpaths, dates_ls)\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "895eb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filenames_dict(rtc_path, variables_ls):\n",
    "\n",
    "    keys, filepaths, dates = [],[],[]\n",
    "    for variable in variables_ls:\n",
    "        keys.append(variable)\n",
    "\n",
    "        filespaths_list, dates_list = make_filepath_lists(rtc_path, variable)\n",
    "        filepaths.append(filespaths_list)\n",
    "        dates.append(dates_list)\n",
    "    \n",
    "    #make dict of variable names (keys) and associated filepaths\n",
    "    filepaths_dict = dict(zip(keys, filepaths))\n",
    "\n",
    "    #make sure that dates are identical across all lists\n",
    "    assert all(lst == dates[0] for lst in dates) == True\n",
    "    #make sure length of each variable list is the same \n",
    "    assert len(list(set([len(v) for k,v in filepaths_dict.items()]))) == 1\n",
    "    \n",
    "\n",
    "\n",
    "    #make dict of variable names (keys) and associated filepaths\n",
    "    filepaths_dict = dict(zip(keys, filepaths))\n",
    "    return filepaths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a0ce344",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths_dict = create_filenames_dict(s1_asf_data, ['vv','vh','ls_map','readme'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb98694",
   "metadata": {},
   "source": [
    "`filepaths_dict` has a key for each file type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f0deba-fecc-491b-b52c-f1748e432fd8",
   "metadata": {},
   "source": [
    "## Read in files using `xr.open_mfdataset()`\n",
    "\n",
    "The `xr.open_mfdataset()` function reads multiple files (in a directory or from a list) and combines them to return a single `xr.Dataset` or `xr.DataArray`. To use the function, specify parameters such as how the files should be combined as well as any preprocessing to execute on the original files. This example will demonstrate a workflow for using `open_mfdataset()` to read in three stacks of roughly 100 RTC images.\n",
    "\n",
    "`preprocess_vv`, `preprocess_vh`  and `preprocess_ls` are identical but for vv and vh bands (read from different tif files). `preprocess_ls` reads in the layover shadow masks that are provided with each scene and similarly stored as tiff files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths_dict['vv'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17dc2220-8d5d-47d2-b5c8-c9f49f0b6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = xr.open_dataset(filepaths_dict['vv'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22fb00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fname_metadata(input_fname: str) -> dict:\n",
    "    \"\"\"Function to extract information from filename and separate into expected variables based on a defined schema.\"\"\"\n",
    "    # Define schema\n",
    "    schema = {\n",
    "        \"sensor\": (3, r\"S1[A-B]\"),  # schema for sensor\n",
    "        \"beam_mode\": (2, r\"[A-Z]{2}\"),  # schema for beam mode\n",
    "        \"acq_date\": (15, r\"[0-9]{8}T[0-9]{6}\"),  # schema for acquisition date\n",
    "        \"pol_orbit\": (3, r\"[A-Z]{3}\"),  # schema for polarization + orbit type\n",
    "        \"terrain_correction_pixel_spacing\": (\n",
    "            5,\n",
    "            r\"RTC[0-9]{2}\",\n",
    "        ),  # schema for terrain correction pixel spacing\n",
    "        \"processing_software\": (\n",
    "            1,\n",
    "            r\"[A-Z]{1}\",\n",
    "        ),  # schema for processing software (G = Gamma)\n",
    "        \"output_info\": (6, r\"[a-z]{6}\"),  # schema for output info\n",
    "        \"product_id\": (4, r\"[A-Z0-9]{4}\"),  # schema for product id\n",
    "        \"prod_type\": ((2, 6), (r\"[A-Z]{2}\", r\"ls_map\")),  # schema for polarization type\n",
    "    }\n",
    "\n",
    "    # Remove prefixs\n",
    "    input_fname = input_fname.split(\"/\")[-1]\n",
    "    # Remove file extension if present\n",
    "    input_fname = input_fname.removesuffix(\".tif\")\n",
    "    # Split filename string into parts\n",
    "    parts = input_fname.split(\"_\")\n",
    "\n",
    "    # l-s map objects have an extra '_' in the filename. Remove/combine parts so that it matches schema\n",
    "    if parts[-1] == \"map\":\n",
    "        parts = parts[:-1]\n",
    "        parts[-1] = parts[-1] + \"_map\"\n",
    "\n",
    "    # Check that number of parts matches expected schema\n",
    "    if len(parts) != len(schema):\n",
    "        raise ValueError(\n",
    "            f\"Input filename does not match schema of expected format: {parts}\"\n",
    "        )\n",
    "\n",
    "    # Create dict to store parsed data\n",
    "    parsed_data = {}\n",
    "\n",
    "    # Iterate through parts and schema\n",
    "    for part, (name, (length_options, pattern_options)) in zip(parts, schema.items()):\n",
    "        # In the schema we defined, items have an int for length or a tuple (when there is more than one possible lenght)\n",
    "        # Make the int lengths into tuples\n",
    "        if isinstance(length_options, int):\n",
    "            length_options = (length_options,)\n",
    "        # Same as above for patterns\n",
    "        if isinstance(pattern_options, str):\n",
    "            pattern_options = (pattern_options,)\n",
    "\n",
    "        # Check that each length of each part matches expected length from schema\n",
    "        if len(part) not in length_options:\n",
    "            raise ValueError(f\"Part {part} does not have expected length {len(part)}\")\n",
    "        # Check that each part matches expected pattern from schema\n",
    "        if not any(re.fullmatch(pattern, part) for pattern in pattern_options):\n",
    "            raise ValueError(\n",
    "                f\"Part {part} does not match expected patterns {pattern_options}\"\n",
    "            )\n",
    "\n",
    "        # Special handling of a part (pol orbit) that has 3 types of metadata\n",
    "        if name == \"pol_orbit\":\n",
    "            parsed_data.update(\n",
    "                {\n",
    "                    \"polarization_type\": part[:1],  # Single (S) or Dual (D) pol\n",
    "                    \"primary_polarization\": part[1:2],  # Primary polarization (H or V)\n",
    "                    \"orbit_type\": part[\n",
    "                        -1\n",
    "                    ],  # Precise (p), Restituted (r) or Original predicted (o)\n",
    "                }\n",
    "            )\n",
    "        # Format string acquisition date as a datetime time stamp\n",
    "        elif name == \"acq_date\":\n",
    "            parsed_data[name] = pd.to_datetime(part, format=\"%Y%m%dT%H%M%S\")\n",
    "        # Expand multiple variables stored in output_info string part\n",
    "        elif name == \"output_info\":\n",
    "            output_info_keys = [\n",
    "                \"output_type\",\n",
    "                \"output_unit\",\n",
    "                \"unmasked_or_watermasked\",\n",
    "                \"notfiltered_or_filtered\",\n",
    "                \"area_or_clipped\",\n",
    "                \"deadreckoning_or_demmatch\",\n",
    "            ]\n",
    "\n",
    "            output_info_values = [part[0], part[1], part[2], part[3], part[4], part[-1]]\n",
    "\n",
    "            parsed_data.update(dict(zip(output_info_keys, output_info_values)))\n",
    "\n",
    "        else:\n",
    "            parsed_data[name] = part\n",
    "\n",
    "    # Because we have already addressed product type in the variable names\n",
    "    prod_type = parsed_data.pop(\"prod_type\")\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68f6fcdf-5fff-4da8-9392-e51fe2f47b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_vv(da_orig):\n",
    "    \"\"\"function that should return an xarray object with time dimension and associated metadata given a path to a single RTC scene, if its dualpol will have multiple bands, currently just as 2 data arrays but could merge.\n",
    "    goal would be to apply this a list of directories for different RTC products, return cube along time dimension - I think?\n",
    "    - for concatenating, would need to check footprints and only take products with the same footprint, or subset them all to a common AOI?\"\"\"\n",
    "\n",
    "    da = da_orig.copy()\n",
    "    da = da.rename({\"band_data\": \"vv\"}).squeeze()\n",
    "\n",
    "    # we want to extract important metadata from the tif filenames\n",
    "    # use the 'source' key from the encoding dictionary to attach the filename\n",
    "    vv_fn = os.path.basename(da_orig[\"band_data\"].encoding[\"source\"])  # [113:]\n",
    "\n",
    "    \n",
    "    attrs_dict = parse_fname_metadata(vv_fn)\n",
    "\n",
    "    # link the strings for each of the above variables to their full names (from README, commented above)\n",
    "    # eg if output_type=g, should read 'gamma'\n",
    "\n",
    "    da.attrs = attrs_dict\n",
    "\n",
    "    utm_zone = da.spatial_ref.attrs[\"crs_wkt\"][17:29]\n",
    "    epsg_code = da.spatial_ref.attrs[\"crs_wkt\"][589:594]\n",
    "\n",
    "    da.attrs[\"utm_zone\"] = utm_zone\n",
    "    da.attrs[\"epsg_code\"] = f\"EPSG:{epsg_code}\"\n",
    "\n",
    "    date = da.attrs[\"acq_date\"]\n",
    "\n",
    "    da = da.assign_coords({\"acq_date\": date})\n",
    "    da = da.expand_dims(\"acq_date\")\n",
    "    da = da.drop_duplicates(dim=[\"x\", \"y\"])\n",
    "\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c3cb5b-c6bd-46ad-b6e2-c3c531a6cebb",
   "metadata": {},
   "source": [
    "Repeat for the other two variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0dc8b82-1f06-45f7-a83b-ef498af6cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_vh(da_orig):\n",
    "    \"\"\"function that should return an xarray object with time dimension and associated metadata given a path to a single RTC scene, if its dualpol will have multiple bands, currently just as 2 data arrays but could merge.\n",
    "    goal would be to apply this a list of directories for different RTC products, return cube along time dimension - I think?\n",
    "    - for concatenating, would need to check footprints and only take products with the same footprint, or subset them all to a common AOI?\"\"\"\n",
    "\n",
    "    # fname_ls = []\n",
    "\n",
    "    da = da_orig.copy()\n",
    "    da = da.rename({\"band_data\": \"vh\"}).squeeze()\n",
    "\n",
    "    # vv_fn = da_orig.encoding['source'][113:]\n",
    "    vh_fn = os.path.basename(da_orig[\"band_data\"].encoding[\"source\"])\n",
    "    # print('fname: ', vv_fn)\n",
    "\n",
    "    attrs_dict = parse_fname_metadata(vh_fn)\n",
    "\n",
    "\n",
    "    # link the strings for each of the above variables to their full names (from README, commented above)\n",
    "    # eg if output_type=g, should read 'gamma'\n",
    "\n",
    "    da.attrs = attrs_dict\n",
    "\n",
    "    utm_zone = da.spatial_ref.attrs[\"crs_wkt\"][17:29]\n",
    "    epsg_code = da.spatial_ref.attrs[\"crs_wkt\"][589:594]\n",
    "\n",
    "    da.attrs[\"utm_zone\"] = utm_zone\n",
    "    da.attrs[\"epsg_code\"] = f\"EPSG:{epsg_code}\"\n",
    "\n",
    "    date = da.attrs[\"acq_date\"]\n",
    "\n",
    "    da = da.assign_coords({\"acq_date\": date})\n",
    "    da = da.expand_dims(\"acq_date\")\n",
    "    da = da.drop_duplicates(dim=[\"x\", \"y\"])\n",
    "\n",
    "    return da  # , fname_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "694a19b1-e7ca-44cb-994a-cc210dacd739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ls(da_orig):\n",
    "    \"\"\"function that should return an xarray object with time dimension and associated metadata given a path to a single RTC scene, if its dualpol will have multiple bands, currently just as 2 data arrays but could merge.\n",
    "    goal would be to apply this a list of directories for different RTC products, return cube along time dimension - I think?\n",
    "    - for concatenating, would need to check footprints and only take products with the same footprint, or subset them all to a common AOI?\"\"\"\n",
    "\n",
    "    da = da_orig.copy()\n",
    "    da = da.rename({\"band_data\": \"layover_shadow_mask\"}).squeeze()\n",
    "\n",
    "    os.path.basename(da_orig[\"band_data\"].encoding[\"source\"])\n",
    "    # print(vv_fn)\n",
    "\n",
    "    attrs_dict = parse_fname_metadata(ls_fn)\n",
    "\n",
    "    # link the strings for each of the above variables to their full names (from README, commented above)\n",
    "    # eg if output_type=g, should read 'gamma'\n",
    "\n",
    "    da.attrs = attrs_dict\n",
    "\n",
    "    utm_zone = da.spatial_ref.attrs[\"crs_wkt\"][17:29]\n",
    "    epsg_code = da.spatial_ref.attrs[\"crs_wkt\"][589:594]\n",
    "\n",
    "    da.attrs[\"utm_zone\"] = utm_zone\n",
    "    da.attrs[\"epsg_code\"] = f\"EPSG:{epsg_code}\"\n",
    "\n",
    "    date = da.attrs[\"acq_date\"]\n",
    "\n",
    "    da = da.assign_coords({\"acq_date\": date})\n",
    "    da = da.expand_dims(\"acq_date\")\n",
    "    da = da.drop_duplicates(dim=[\"x\", \"y\"])\n",
    "\n",
    "    # vec = gpd.read_file('https://github.com/e-marshall/s1_book/raw/main/data/hma_lakes_aoi.geojson')\n",
    "    # print(vec.crs)\n",
    "    # da_clip = da.rio.clip(vec.geometry, vec.crs, drop=True)\n",
    "    # print(da_clip.crs)\n",
    "\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6e8db",
   "metadata": {},
   "source": [
    "### An example of complicated chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c022c13-c49f-4b56-9463-49843294c934",
   "metadata": {},
   "source": [
    "First, let's call `xr.open_mfdataset()` with the argument `chunks='auto'`. This will read in a dask array where ideal chunk sizes are selected based off the array size, it will attempt to have chunk sizes where bytes are equal to the configuration value for array chunk size. More about that [here](https://docs.dask.org/en/stable/array-chunks.html#automatic-chunking). You can check the configuration value for an array chunk size with the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b64b1-485c-4544-89dc-c96a4f64300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.get(\"array.chunk-size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f8d1791-174f-4907-8a4a-085fc24fb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_vh = xr.open_mfdataset(\n",
    "    paths=filepaths_dict['vh'],\n",
    "    preprocess=preprocess_vh,\n",
    "    chunks=\"auto\",\n",
    "    engine=\"rasterio\",\n",
    "    data_vars=\"minimal\",\n",
    "    coords=\"minimal\",\n",
    "    concat_dim=\"acq_date\",\n",
    "    combine=\"nested\",\n",
    "    parallel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1eb1c-04ee-41fb-ba78-e2e30621cf58",
   "metadata": {},
   "source": [
    "If we take a look at the `asf_vh` object we just created (click the stack icon on the right of the `vh` variable tab), we see that the chunking is quite complicated. This isn't ideal because it can create problems like excessive communication between workers (and a lot of memory usage) down the line when we perform non-lazy operations. It seems like the `'auto'` chunking is applied to the top layer (file) of the dataset, but because the spatial footprint of each file is not the same, the 'auto' chunking does not persist through layers and we end up with the funky layout in the object above. \n",
    "\n",
    "Let's create both the VV and VH objects with `chunks=None` and see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af699b09-5717-4120-a365-7d5c7044b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_vh = xr.open_mfdataset(\n",
    "    paths=filepaths_dict['vh'],\n",
    "    preprocess=preprocess_vh,\n",
    "    chunks=None,\n",
    "    engine=\"rasterio\",\n",
    "    data_vars=\"minimal\",\n",
    "    coords=\"minimal\",\n",
    "    concat_dim=\"acq_date\",\n",
    "    combine=\"nested\",\n",
    "    parallel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef7125-41ac-4e1f-a084-02720d7d8cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d031d86-1a0b-4543-820a-01045644b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_vv = xr.open_mfdataset(\n",
    "    paths=fpaths_vv,\n",
    "    preprocess=preprocess_vv,\n",
    "    chunks=None,\n",
    "    engine=\"rasterio\",\n",
    "    data_vars=\"minimal\",\n",
    "    coords=\"minimal\",\n",
    "    concat_dim=\"acq_date\",\n",
    "    combine=\"nested\",\n",
    "    parallel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ee717-7476-4024-9281-37992e6d6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_ls = xr.open_mfdataset(\n",
    "    paths=fpaths_ls,\n",
    "    preprocess=preprocess_ls,\n",
    "    chunks=None,\n",
    "    engine=\"rasterio\",\n",
    "    data_vars=\"minimal\",\n",
    "    coords=\"minimal\",\n",
    "    concat_dim=\"acq_date\",\n",
    "    combine=\"nested\",\n",
    "    parallel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a42655-3bc9-43aa-8b69-9108e02dce60",
   "metadata": {},
   "source": [
    "Now our chunks are quite large, but they are at least in a format that makes more sense for our dataset. This will make it easier to clip the object to our area of interest, which will solve the problem of large chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f729f9-4ec5-4e76-8adf-e86e5b85b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c7579-4017-42b0-9e1b-a3e098d62a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14865e80-830a-4658-acc7-27128b177334",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbded1-a7a7-4774-9eed-2152274efe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_ls.acq_date.equals(asf_vv.acq_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff986c-c55f-4400-93ed-4daae55057b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(asf_ls.acq_date[0].values)\n",
    "print(asf_vv.acq_date[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ee5f4-eac3-4c28-a978-77d2674e849d",
   "metadata": {},
   "source": [
    "Merge the VH, VV and layover-shadow mask objects into one `xr.Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8312220-60f4-4f99-941d-aea8644b8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_ds = xr.Dataset(\n",
    "    {\"vv\": asf_vv.vv, \"vh\": asf_vh.vh, \"ls\": asf_ls.layover_shadow_mask}\n",
    ")\n",
    "asf_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0814c-5996-44d0-86c3-5185fed0cc0a",
   "metadata": {},
   "source": [
    "If you take a look at the `acq_date` coordinate, you will see that they are not in order. Let's sort by the `acq_date` (time) dimension using xarray `.sortby()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1ae58-8098-40a9-b1a5-16dfc2d95d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_ds_sorted = asf_ds.sortby(asf_ds.acq_date)\n",
    "asf_ds_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9c83a-4587-489b-b42a-468286d97da8",
   "metadata": {},
   "source": [
    "## Clip stack to AOI using `rioxarray.clip()`\n",
    "\n",
    "This is a pretty unwieldy object (nearly 200 GB). Let's subset it down to just the area we want to focus on. Read in the following GeoJSON to clip the dataset to the area of interest we'll be using in this tutorial. This AOI covers a region in the central Himalaya near the Chinese border. We will use the `rioxarray.clip()` function which you can read more about [here](https://corteva.github.io/rioxarray/stable/examples/clip_geom.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac043141-d3f8-4ac4-a68a-b79f0c8e9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_aoi = gpd.read_file(\n",
    "    \"https://github.com/e-marshall/sentinel1_rtc/raw/main/hma_rtc_aoi.geojson\"\n",
    ")\n",
    "pc_aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b9d57-defe-4d31-9ce5-3f8436de581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_aoi.plot(facecolor=\"None\", edgecolor=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c471d-00a6-4b15-8d0e-73d02d6d2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_clip = asf_ds_sorted.rio.clip(pc_aoi.geometry, pc_aoi.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe9249-b6c4-4c47-b159-476196dfdb10",
   "metadata": {},
   "source": [
    "Check the size of the clipped object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbd96d-a06a-42f2-9f82-377f99ec9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "asf_clip.nbytes / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca3b5a-7c1a-4b4a-97fc-22c8dd7bd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't want to compute this right now, will kill kernel\n",
    "# asf_clip_load = asf_clip.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8dd4f-dd2a-4048-82db-4271057a9d13",
   "metadata": {},
   "source": [
    "The dataset is published as gamma-nought values on the power scale. This is useful for performing statistical analysis but not always for visualization. We convert to decibel scale (sigma-nought) in order to visualize variability in backscatter more easily. Read more about scales used to represent SAR data [here](https://hyp3-docs.asf.alaska.edu/guides/rtc_product_guide/#sar-scales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f982c-a421-4423-a70b-18dcadc46a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_to_db(input_arr):\n",
    "    return 10 * np.log10(np.abs(input_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40daae69-def7-4e5d-a0be-0c73eda778d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(18, 7))\n",
    "\n",
    "asf_clip.isel(acq_date=10).ls.plot(ax=axs[0])\n",
    "power_to_db(asf_clip.isel(acq_date=10).vv).plot(ax=axs[1], cmap=plt.cm.Greys_r)\n",
    "power_to_db(asf_clip.isel(acq_date=10).vh).plot(ax=axs[2], cmap=plt.cm.Greys_r)\n",
    "fig.suptitle(\n",
    "    f\"Layover-shadow mask (L), VV (C) and VH (R) backscatter on {str(asf_clip.isel(acq_date=10).acq_date.data)[:-19]}\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581daf8-51fd-4164-838e-c39b0a0ca590",
   "metadata": {},
   "source": [
    "## Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e55abe-8a2d-433d-9619-26bb8ea1d2f9",
   "metadata": {},
   "source": [
    "The `xr.open_mfdataset()` approach is very useful for reading in large amounts of data from a number of distributed files very efficiently. It's especially helpful that the `preprocess()` function allows a large degree of customization for how the data is read in, and that the appropriate metadata is preserved as individual `xr.DataArrays` are organized into data cubes and we have to do very little further work of organizing metadata. \n",
    "\n",
    "However, due to the nature of this stack of data, where each time-element of the stack covers a common region of interest but does not have a uniform spatial footprint, this approach is not very computationally efficient. We are reading in a vast footprint of data to subset it to a much smaller area. While it is able to execute on this machine, a more computationally-intensive workflow would fail. \n",
    "\n",
    "Another approach we can try is to use GDAL VRT objects. GDAL VRT objects create an xml file from a list of GeoTIFF files. The xml contains a mapping of all of the specified raster files so that we essentially have the spatial information that would have been used to create the full data object, and all of the information we need for a clip. VRT objects are able to handle the mismatch of grids within the stack of files, so does not encounter the memory issues that we get when trying the `xr.open_mfdataset()` approach with dask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3dc49e-03b3-47e2-9d21-b470dd5ad33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial_datacube_book_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
